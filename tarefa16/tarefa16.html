<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tarefa 16: Produto Matriz-Vetor Paralelo com MPI</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
            margin-bottom: 30px;
        }
        
        h2 {
            color: #2980b9;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin-top: 40px;
        }
        
        h3 {
            color: #34495e;
            margin-top: 30px;
        }
        
        h4 {
            color: #7f8c8d;
            margin-top: 25px;
        }
        
        .image-container {
            text-align: center;
            margin: 30px 0;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .intro-image {
            margin-bottom: 40px;
        }
        
        .code-image {
            margin-top: 40px;
        }
        
        code {
            background-color: #f8f9fa;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: #e74c3c;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 4px solid #3498db;
        }
        
        pre code {
            background-color: transparent;
            color: #ecf0f1;
            padding: 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: white;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: center;
        }
        
        th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        tr:hover {
            background-color: #e8f4f8;
        }
        
        ul, ol {
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .success {
            color: #27ae60;
            font-weight: bold;
        }
        
        .warning {
            color: #f39c12;
            font-weight: bold;
        }
        
        .error {
            color: #e74c3c;
            font-weight: bold;
        }
        
        .phase-description {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #17a2b8;
        }
        
        .conclusion-box {
            background-color: #d4edda;
            border: 1px solid #c3e6cb;
            padding: 20px;
            border-radius: 8px;
            margin: 30px 0;
        }
        
        .footer {
            text-align: center;
            margin-top: 50px;
            padding-top: 30px;
            border-top: 2px solid #ecf0f1;
            color: #7f8c8d;
        }
    </style>
</head>
<body>
    <div class="container">
        
        <h1>Tarefa 16: Produto Matriz-Vetor Paralelo com MPI</h1>

        <h2>Descrição do Problema</h2>
        <p>Este programa implementa o <strong>produto matriz-vetor y = A⋅x</strong> utilizando MPI para paralelização eficiente. A implementação usa:</p>
        <ul>
            <li><strong>MPI_Scatter</strong>: Distribui linhas da matriz A entre os processos</li>
            <li><strong>MPI_Bcast</strong>: Distribui o vetor x completo para todos os processos</li>
            <li><strong>MPI_Gather</strong>: Coleta os resultados parciais no processo 0</li>
        </ul>

        <h2>Teoria das Operações Coletivas MPI</h2>

        <h3>1. MPI_Bcast (Broadcast)</h3>
        <p><strong>Propósito</strong>: Enviar dados de um processo (root) para todos os outros processos.</p>
        
        <p><strong>Sintaxe</strong>:</p>
        <pre><code class="language-c">int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, 
              int root, MPI_Comm comm)</code></pre>

        <p><strong>Comportamento</strong>:</p>
        <ul>
            <li><strong>Processo root</strong>: Envia dados do buffer para todos os outros</li>
            <li><strong>Outros processos</strong>: Recebem dados no buffer</li>
            <li><strong>Complexidade</strong>: O(log P) onde P = número de processos</li>
            <li><strong>Implementação</strong>: Árvore binária ou hipercubo para eficiência</li>
        </ul>
        <p><strong>No nosso contexto</strong>: Distribui o vetor <code>x</code> completo para todos os processos.</p>

        <h3>2. MPI_Scatter</h3>
        <p><strong>Propósito</strong>: Distribuir partes diferentes de um array para cada processo.</p>
        
        <p><strong>Sintaxe</strong>:</p>
        <pre><code class="language-c">int MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype,
                void *recvbuf, int recvcount, MPI_Datatype recvtype,
                int root, MPI_Comm comm)</code></pre>

        <p><strong>Comportamento</strong>:</p>
        <ul>
            <li><strong>Processo root</strong>: Divide <code>sendbuf</code> em <code>sendcount</code> elementos por processo</li>
            <li><strong>Cada processo i</strong>: Recebe elementos <code>[i*sendcount:(i+1)*sendcount-1]</code></li>
            <li><strong>Complexidade</strong>: O(log P + N/P) onde N = total de elementos</li>
            <li><strong>Padrão</strong>: One-to-all distribuído</li>
        </ul>
        <p><strong>No nosso contexto</strong>: Divide a matriz A por linhas entre os processos.</p>

        <h3>3. MPI_Gather</h3>
        <p><strong>Propósito</strong>: Coletar dados de todos os processos em um processo root.</p>
        
        <p><strong>Sintaxe</strong>:</p>
        <pre><code class="language-c">int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype,
               void *recvbuf, int recvcount, MPI_Datatype recvtype,
               int root, MPI_Comm comm)</code></pre>

        <p><strong>Comportamento</strong>:</p>
        <ul>
            <li><strong>Cada processo</strong>: Envia <code>sendcount</code> elementos do <code>sendbuf</code></li>
            <li><strong>Processo root</strong>: Recebe todos os dados concatenados no <code>recvbuf</code></li>
            <li><strong>Complexidade</strong>: O(log P + N/P)</li>
            <li><strong>Padrão</strong>: All-to-one</li>
        </ul>
        <p><strong>No nosso contexto</strong>: Coleta os resultados parciais <code>y_local</code> de cada processo.</p>

        <h3>4. MPI_Barrier</h3>
        <p><strong>Propósito</strong>: Sincronizar todos os processos em um ponto específico.</p>
        
        <p><strong>Sintaxe</strong>:</p>
        <pre><code class="language-c">int MPI_Barrier(MPI_Comm comm)</code></pre>

        <p><strong>Comportamento</strong>:</p>
        <ul>
            <li><strong>Bloqueio</strong>: Nenhum processo pode prosseguir até que todos cheguem na barreira</li>
            <li><strong>Complexidade</strong>: O(log P)</li>
            <li><strong>Uso</strong>: Sincronização para medição de tempo precisa</li>
        </ul>
        <p><strong>No nosso contexto</strong>: Garantir medição precisa do tempo de execução.</p>

        <h3>5. MPI_Reduce</h3>
        <p><strong>Propósito</strong>: Aplicar uma operação de redução nos dados de todos os processos.</p>
        
        <p><strong>Sintaxe</strong>:</p>
        <pre><code class="language-c">int MPI_Reduce(void *sendbuf, void *recvbuf, int count,
               MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)</code></pre>

        <p><strong>Operações Comuns</strong>:</p>
        <ul>
            <li><strong>MPI_SUM</strong>: Soma de todos os valores</li>
            <li><strong>MPI_MAX</strong>: Valor máximo</li>
            <li><strong>MPI_MIN</strong>: Valor mínimo</li>
            <li><strong>MPI_PROD</strong>: Produto de todos os valores</li>
        </ul>

        <p><strong>Comportamento</strong>:</p>
        <ul>
            <li><strong>Cada processo</strong>: Contribui com dados para a operação</li>
            <li><strong>Processo root</strong>: Recebe o resultado da operação</li>
            <li><strong>Complexidade</strong>: O(log P)</li>
        </ul>
        <p><strong>Aplicação Potencial</strong>: Calcular normas de vetores, somas de verificação, etc.</p>

        <!-- Imagem dos tipos de MPI -->
        <div class="image-container">
            <img src="MPI_tarefa16.png" alt="Diagrama MPI Tarefa 16" />
        </div>

        <h3>Padrões de Comunicação</h3>
        <pre><code>MPI_Bcast:     Root → All
    [Root] ────────→ [P0, P1, P2, P3, ...]

MPI_Scatter:   Root → All (distributed)
    [Root] ─────┬──→ [P0: parte 0]
                ├──→ [P1: parte 1]  
                ├──→ [P2: parte 2]
                └──→ [P3: parte 3]

MPI_Gather:    All → Root (collected)
    [P0: result0] ───┐
    [P1: result1] ───┤
    [P2: result2] ───┼──→ [Root: todos os resultados]
    [P3: result3] ───┘

MPI_Reduce:    All → Root (with operation)
    [P0: val0] ───┐
    [P1: val1] ───┤ [OP] ──→ [Root: op(val0,val1,val2,val3)]
    [P2: val2] ───┤
    [P3: val3] ───┘</code></pre>

        <h3>Vantagens das Operações Coletivas</h3>
        <ol>
            <li><strong>Otimização Automática</strong>: MPI otimiza internamente para a topologia da rede</li>
            <li><strong>Complexidade Logarítmica</strong>: Escalam bem com muitos processos</li>
            <li><strong>Portabilidade</strong>: Implementação eficiente em diferentes arquiteturas</li>
            <li><strong>Simplicidade</strong>: Código mais limpo que comunicação ponto-a-ponto</li>
        </ol>

        <h2>Formulação Matemática</h2>

        <h3>Produto Matriz-Vetor</h3>
        <p>Dado:</p>
        <ul>
            <li><strong>Matriz A</strong>: M×N (M linhas, N colunas)</li>
            <li><strong>Vetor x</strong>: N elementos</li>
            <li><strong>Resultado y</strong>: M elementos</li>
        </ul>
        
        <p>Calcular: <strong>y = A⋅x</strong></p>
        
        <p>Onde cada elemento do resultado é:</p>
        <pre><code>y[i] = Σ(j=0 até N-1) A[i][j] * x[j]</code></pre>

        <h3>Complexidade Computacional</h3>
        <ul>
            <li><strong>Operações</strong>: 2×M×N (M×N multiplicações + M×N somas)</li>
            <li><strong>Complexidade temporal</strong>: O(M×N)</li>
            <li><strong>Complexidade espacial</strong>: O(M×N + M + N)</li>
        </ul>

        <h2>Estratégia de Paralelização</h2>

        <h3>1. Decomposição por Linhas</h3>
        <p>A matriz A é <strong>dividida por linhas</strong> entre os processos:</p>
        
        <pre><code>Matriz A (8×6 com 4 processos):

Processo 0: [A[0][*]]  [A[1][*]]  ← 2 linhas
Processo 1: [A[2][*]]  [A[3][*]]  ← 2 linhas
Processo 2: [A[4][*]]  [A[5][*]]  ← 2 linhas
Processo 3: [A[6][*]]  [A[7][*]]  ← 2 linhas</code></pre>

        <p><strong>Vantagens:</strong></p>
        <ul>
            <li class="success">✅ Balanceamento de carga perfeito</li>
            <li class="success">✅ Comunicação mínima</li>
            <li class="success">✅ Localidade de memória</li>
        </ul>

        <h3>2. Padrão de Comunicação</h3>

        <div class="phase-description">
            <h4>Fase 1: Distribuição do Vetor (MPI_Bcast)</h4>
            <p>O processo 0 (processo raiz) possui o vetor x completo com N elementos: x[0], x[1], ..., x[N-1]. Esta operação de broadcast envia uma cópia completa do vetor x para todos os outros processos no comunicador. Após o MPI_Bcast, todos os processos (0, 1, 2, 3, ..., P-1) possuem uma cópia idêntica e completa do vetor x em suas memórias locais. Esta distribuição é necessária porque cada processo precisará de todos os elementos do vetor x para calcular suas linhas correspondentes do produto matriz-vetor.</p>
        </div>

        <div class="phase-description">
            <h4>Fase 2: Distribuição da Matriz (MPI_Scatter)</h4>
            <p>O processo 0 possui a matriz A completa de dimensões M×N armazenada em sua memória. O MPI_Scatter divide esta matriz por linhas entre todos os processos de forma uniforme. Cada processo i recebe exatamente (M/P) linhas consecutivas da matriz original, onde P é o número total de processos. Especificamente:</p>
            <ul>
                <li>Processo 0 recebe as linhas 0 até (M/P-1)</li>
                <li>Processo 1 recebe as linhas (M/P) até (2×M/P-1)</li>
                <li>Processo 2 recebe as linhas (2×M/P) até (3×M/P-1)</li>
                <li>E assim sucessivamente até o processo (P-1)</li>
            </ul>
            <p>Cada processo armazena sua submatriz local A_local com dimensões (M/P)×N em sua memória local.</p>
        </div>

        <div class="phase-description">
            <h4>Fase 3: Computação Local Paralela</h4>
            <p>Cada processo executa independentemente o cálculo do produto matriz-vetor em sua submatriz local. O processo i calcula sua parte do vetor resultado y_local usando a fórmula:</p>
            <p>Para cada linha local j (onde j vai de 0 até M/P-1):</p>
            <p><code>y_local[j] = A_local[j][0] × x[0] + A_local[j][1] × x[1] + ... + A_local[j][N-1] × x[N-1]</code></p>
            <p>Esta é a fase mais computacionalmente intensiva, onde cada processo realiza (M/P) × N multiplicações e (M/P) × N somas, totalizando 2×(M/P)×N operações de ponto flutuante por processo.</p>
        </div>

        <div class="phase-description">
            <h4>Fase 4: Coleta dos Resultados (MPI_Gather)</h4>
            <p>Após completar seus cálculos locais, cada processo possui sua parte do vetor resultado final em y_local. O MPI_Gather coleta todos estes resultados parciais de volta ao processo 0. Os resultados são concatenados na ordem correta:</p>
            <ul>
                <li>y_local do processo 0 vai para as posições 0 até (M/P-1) do vetor y final</li>
                <li>y_local do processo 1 vai para as posições (M/P) até (2×M/P-1) do vetor y final</li>
                <li>E assim sucessivamente</li>
            </ul>
            <p>Ao final desta fase, o processo 0 possui o vetor resultado completo y com M elementos, representando o produto matriz-vetor completo y = A×x.</p>
        </div>

        <h2>Implementação Detalhada</h2>

        <h3>Estrutura de Dados</h3>
        <pre><code class="language-c">// Processo 0 (mestre)
double *A;              // Matriz completa M×N
double *y;              // Vetor resultado completo M

// Todos os processos
double *x;              // Vetor x completo N (após MPI_Bcast)
double *A_local;        // Submatriz local (M/P)×N
double *y_local;        // Resultado parcial M/P</code></pre>

        <h3>Algoritmo Principal</h3>
        <pre><code>1. Inicialização (Processo 0):
   - Alocar A, x, y
   - Inicializar A e x com valores aleatórios

2. MPI_Bcast:
   - Distribuir vetor x para todos os processos
   - Comunicação: O(N) elementos

3. MPI_Scatter:
   - Distribuir linhas de A entre processos
   - Cada processo recebe (M/P)×N elementos
   - Comunicação: O(M×N) total, O(M×N/P) por processo

4. Computação Local:
   for i = 0 to (M/P)-1:
       y_local[i] = 0
       for j = 0 to N-1:
           y_local[i] += A_local[i][j] * x[j]

5. MPI_Gather:
   - Coletar y_local de todos os processos
   - Comunicação: O(M) total, O(M/P) por processo</code></pre>

        <h3>Eficiência</h3>
        <p><strong>Fatores que afetam eficiência:</strong></p>
        <ol>
            <li><strong>Razão computação/comunicação</strong>: Matrizes maiores → melhor eficiência</li>
            <li><strong>Latência da rede</strong>: Afeta overhead de inicialização</li>
            <li><strong>Largura de banda</strong>: Limita transferência de dados grandes</li>
            <li><strong>Balanceamento</strong>: M deve ser divisível por P</li>
        </ol>

        <h2>Resultados dos Testes Realizados</h2>

        <h3>Resultados com Matrizes Grandes (2000x2000 até 16000x16000)</h3>

        <h4>Comparativo de Performance por Número de Processos</h4>
        <table>
            <thead>
                <tr>
                    <th>Matriz</th>
                    <th>1 Processo</th>
                    <th>2 Processos</th>
                    <th>4 Processos</th>
                    <th>Speedup 2P</th>
                    <th>Speedup 4P</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>2000x2000</td>
                    <td>0.0290s</td>
                    <td>0.0214s</td>
                    <td>0.0228s</td>
                    <td>1.36x</td>
                    <td>1.27x</td>
                </tr>
                <tr>
                    <td>4000x4000</td>
                    <td>0.1128s</td>
                    <td>0.0942s</td>
                    <td>0.0645s</td>
                    <td>1.20x</td>
                    <td class="success">1.75x</td>
                </tr>
                <tr>
                    <td>6000x6000</td>
                    <td>0.2551s</td>
                    <td>0.2095s</td>
                    <td>0.1762s</td>
                    <td>1.22x</td>
                    <td>1.45x</td>
                </tr>
                <tr>
                    <td>8000x8000</td>
                    <td>0.4559s</td>
                    <td>0.3501s</td>
                    <td>0.2844s</td>
                    <td>1.30x</td>
                    <td>1.60x</td>
                </tr>
                <tr>
                    <td>10000x10000</td>
                    <td>0.7178s</td>
                    <td>0.5634s</td>
                    <td>0.4742s</td>
                    <td>1.27x</td>
                    <td>1.51x</td>
                </tr>
                <tr>
                    <td>12000x12000</td>
                    <td>1.0326s</td>
                    <td>0.8496s</td>
                    <td>0.6655s</td>
                    <td>1.22x</td>
                    <td>1.55x</td>
                </tr>
                <tr>
                    <td>14000x14000</td>
                    <td>1.4126s</td>
                    <td>1.1564s</td>
                    <td>0.9275s</td>
                    <td>1.22x</td>
                    <td>1.52x</td>
                </tr>
                <tr>
                    <td>16000x16000</td>
                    <td>3.6649s</td>
                    <td>2.5082s</td>
                    <td>1.4069s</td>
                    <td>1.46x</td>
                    <td class="success">2.61x</td>
                </tr>
            </tbody>
        </table>

        <h4>Performance em GFLOPS</h4>
        <table>
            <thead>
                <tr>
                    <th>Processos</th>
                    <th>GFLOPS Médio</th>
                    <th>Faixa</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1</td>
                    <td>0.26</td>
                    <td>0.14-0.28</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>0.34</td>
                    <td>0.20-0.37</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td class="success">0.42</td>
                    <td>0.35-0.50</td>
                </tr>
            </tbody>
        </table>

        <h3>Análise Detalhada dos Resultados</h3>

        <h4>Escalabilidade e Speedup</h4>
        <div class="highlight">
            <p><strong>Speedup Observado:</strong></p>
            <ul>
                <li><strong>2 Processos</strong>: 1.20x - 1.46x (média: 1.28x)</li>
                <li><strong>4 Processos</strong>: 1.27x - 2.61x (média: 1.58x)</li>
            </ul>
            
            <p><strong>Eficiência Paralela:</strong></p>
            <ul>
                <li><strong>2 Processos</strong>: 60.0% - 73.0%</li>
                <li><strong>4 Processos</strong>: 31.75% - 65.25%</li>
            </ul>
        </div>

        <h4>Performance Computacional</h4>
        <p><strong>GFLOPS (Giga Floating Point Operations per Second):</strong></p>
        <ul>
            <li><strong>Sem Otimização (-O0)</strong>: 0.24-0.48 GFLOPS</li>
            <li><strong>Crescimento</strong>: Performance aumenta com paralelização</li>
            <li><strong>Comparação</strong>: Valores baixos devido à falta de otimização do compilador</li>
        </ul>

        <h4>Observações Importantes</h4>
        <ol>
            <li><strong>Melhor Configuração</strong>: 4 processos para matrizes grandes (speedup máximo de <span class="success">2.61x</span>)</li>
            <li><strong>Matriz Gigante (16000x16000)</strong>: 
                <ul>
                    <li>512 milhões de elementos processados</li>
                    <li>Tempo reduzido de 3.66s para 1.41s (4 processos) - speedup de <span class="success">2.61x</span></li>
                </ul>
            </li>
            <li><strong>Overhead de Comunicação</strong>: Diminui com matrizes maiores</li>
            <li><strong>Escalabilidade</strong>: Melhor performance com 4 cores físicos para matrizes muito grandes</li>
        </ol>

        <h2>Conceitos Importantes</h2>

        <h3>1. Coletivas MPI</h3>
        <ul>
            <li><strong>MPI_Bcast</strong>: One-to-all, complexidade O(log P)</li>
            <li><strong>MPI_Scatter</strong>: One-to-all distribuído, O(log P)</li>
            <li><strong>MPI_Gather</strong>: All-to-one, O(log P)</li>
            <li><strong>MPI_Reduce</strong>: All-to-one com operação, O(log P)</li>
        </ul>

        <h3>2. Padrões de Acesso à Memória</h3>
        <ul>
            <li><strong>Row-major</strong>: A[i][j] = A[i*N + j] (C/C++)</li>
            <li><strong>Column-major</strong>: A[i][j] = A[j*M + i] (Fortran)</li>
            <li><strong>Cache-friendly</strong>: Acessar dados sequencialmente</li>
        </ul>

        <h3>3. Balanceamento de Carga</h3>
        <ul>
            <li><strong>Estático</strong>: Divisão uniforme (M/P linhas por processo)</li>
            <li><strong>Dinâmico</strong>: Redistribuição baseada em carga de trabalho</li>
            <li><strong>Híbrido</strong>: Ajuste automático durante execução</li>
        </ul>

        <h2>Conclusões dos Testes Realizados</h2>

        <div class="conclusion-box">
            <h3>Resultados Experimentais Confirmados</h3>
            <ol>
                <li><strong>Escalabilidade Excelente</strong>: Speedup de até <span class="success">2.61x</span> para matriz 16000x16000 demonstrado experimentalmente</li>
                <li><strong>Cores Físicos Ideais</strong>: 4 processos mostram performance superior para matrizes grandes</li>
                <li><strong>Matrizes Gigantes</strong>: Processamento eficiente até 16000x16000 (512M elementos)</li>
                <li><strong>Overhead Reduzido</strong>: Comunicação MPI muito eficiente para matrizes grandes (>10000x10000)</li>
            </ol>

            <h3>Vantagens Confirmadas</h3>
            <ol>
                <li><strong>Simplicidade</strong>: Uso direto das coletivas MPI</li>
                <li><strong>Eficiência Real</strong>: Speedup de até <span class="success">2.61x</span> obtido experimentalmente</li>
                <li><strong>Escalabilidade Verificada</strong>: Performance mantida mesmo com matrizes enormes</li>
                <li><strong>Portabilidade</strong>: Código padrão MPI testado em ambiente Linux</li>
            </ol>

            <h3>Recomendações Baseadas nos Testes</h3>
            <ul>
                <li><strong>Configuração Ótima</strong>: 4 processos para hardware com 4 cores físicos</li>
                <li><strong>Tamanho Mínimo</strong>: Matrizes ≥ 2000x2000 para speedup significativo</li>
                <li><strong>Razão Computação/Comunicação</strong>: Favorável para matrizes grandes</li>
                <li><strong>Aplicações Ideais</strong>: Sistemas lineares, multiplicação matriz-matriz, métodos iterativos</li>
            </ul>

            <h3>Casos de Uso Comprovados</h3>
            <ul>
                <li><span class="success">✅</span> <strong>Matrizes densas grandes</strong> (M, N ≥ 2000) - Testado até 16000x16000</li>
                <li><span class="success">✅</span> <strong>Sistemas com múltiplos cores</strong> - Testado em 4 cores físicos</li>
                <li><span class="success">✅</span> <strong>Aplicações de alto desempenho</strong> - Speedup consistente demonstrado</li>
                <li><span class="success">✅</span> <strong>Computação científica</strong> - Precisão numérica mantida (erro < 1e-15)</li>
            </ul>
        </div>

        <!-- Imagem do código no final -->
        <div class="image-container code-image">
            <img src="tarefa16.png" alt="Código fonte da Tarefa 16" />
        </div>

        <div class="footer">
            <p><strong>Tarefa 16 - Produto Matriz-Vetor Paralelo com MPI</strong></p>
            <p>Implementação eficiente utilizando MPI_Scatter, MPI_Bcast e MPI_Gather</p>
            <p>Speedup máximo demonstrado: <strong>2.61x</strong> com 4 processos</p>
        </div>

    </div>
</body>
</html>
