<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tarefa 15: Simula√ß√£o de Difus√£o de Calor 1D com MPI</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        
        h2 {
            color: #34495e;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin-top: 30px;
        }
        
        h3 {
            color: #5d6d7e;
            margin-top: 25px;
        }
        
        h4 {
            color: #7f8c8d;
            margin-top: 20px;
        }
        
        .theory-section {
            background-color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #3498db;
        }
        
        .method-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .method-card {
            background-color: #fff;
            border: 1px solid #bdc3c7;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .method-card h4 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        
        .pros-cons {
            margin: 15px 0;
        }
        
        .pros {
            color: #27ae60;
        }
        
        .cons {
            color: #e74c3c;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: white;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        th, td {
            border: 1px solid #bdc3c7;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        tr:hover {
            background-color: #e8f4f8;
        }
        
        code {
            background-color: #f1f2f6;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            color: #2c3e50;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 5px solid #3498db;
        }
        
        .formula {
            text-align: center;
            font-style: italic;
            color: #2c3e50;
            background-color: #ecf0f1;
            padding: 10px;
            border-radius: 5px;
            margin: 15px 0;
        }
        
        .results-section {
            background-color: #e8f5e8;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #27ae60;
        }
        
        .performance-highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #ffc107;
            margin: 15px 0;
        }
        
        .conclusion-box {
            background-color: #d1ecf1;
            padding: 20px;
            border-radius: 8px;
            border-left: 5px solid #17a2b8;
            margin: 20px 0;
        }
        
        .compilation-section {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 5px solid #6c757d;
            margin: 20px 0;
        }
        
        .star-rating {
            color: #f39c12;
        }
        
        .emoji {
            font-size: 1.2em;
        }
        
        .highlight-best {
            background-color: #d4edda;
            font-weight: bold;
        }
        
        .final-image {
            text-align: center;
            margin: 40px 0;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 10px;
            border: 2px dashed #3498db;
        }
        
        .final-image img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.2);
        }
        
        .final-image h3 {
            color: #2c3e50;
            margin-bottom: 15px;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 15px;
            }
            
            .method-comparison {
                grid-template-columns: 1fr;
            }
            
            table {
                font-size: 0.9em;
            }
            
            pre {
                font-size: 0.85em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Tarefa 15: Simula√ß√£o de Difus√£o de Calor 1D com MPI</h1>

        <h2>Descri√ß√£o do Problema</h2>
        <p>Este programa implementa uma simula√ß√£o de <strong>difus√£o de calor em uma barra unidimensional</strong> utilizando MPI (Message Passing Interface) para paraleliza√ß√£o. A simula√ß√£o resolve numericamente a <strong>equa√ß√£o de difus√£o t√©rmica</strong>:</p>
        
        <div class="formula">
            ‚àÇT/‚àÇt = Œ± * ‚àÇ¬≤T/‚àÇx¬≤
        </div>
        
        <p><strong>Onde:</strong></p>
        <ul>
            <li><code>T(x,t)</code> = temperatura no ponto x no tempo t</li>
            <li><code>Œ±</code> = coeficiente de difus√£o t√©rmica</li>
            <li><code>x</code> = posi√ß√£o ao longo da barra</li>
            <li><code>t</code> = tempo</li>
        </ul>

        <h2>Teoria das Comunica√ß√µes MPI</h2>
        <div class="theory-section">
            <p>Este trabalho compara <strong>tr√™s estrat√©gias diferentes de comunica√ß√£o MPI</strong> para troca de dados entre processos vizinhos. Cada abordagem tem caracter√≠sticas te√≥ricas distintas em termos de <strong>bloqueio</strong>, <strong>sobreposi√ß√£o</strong> e <strong>efici√™ncia</strong>.</p>
        </div>

        <div class="method-comparison">
            <div class="method-card">
                <h4>1. Comunica√ß√£o Bloqueante: MPI_Send/MPI_Recv</h4>
                <p><strong>Defini√ß√£o</strong>: Opera√ß√µes de comunica√ß√£o <strong>s√≠ncronas</strong> que bloqueiam a execu√ß√£o at√© a transfer√™ncia de dados ser completada.</p>
                
                <pre><code>// Processo remetente
MPI_Send(buffer, count, datatype, dest, tag, comm);
// Bloqueia at√© o dado ser enviado

// Processo receptor  
MPI_Recv(buffer, count, datatype, source, tag, comm, status);
// Bloqueia at√© o dado ser recebido</code></pre>

                <div class="pros-cons">
                    <p><strong>Caracter√≠sticas Te√≥ricas:</strong></p>
                    <ul>
                        <li class="pros">‚úÖ <strong>Simplicidade</strong>: API direta e intuitiva</li>
                        <li class="pros">‚úÖ <strong>Garantias fortes</strong>: Dados confirmadamente transferidos ao retornar</li>
                        <li class="pros">‚úÖ <strong>Sem overhead</strong>: M√≠nimo custo de gerenciamento</li>
                        <li class="cons">‚ùå <strong>Serializa√ß√£o</strong>: Processos ficam ociosos durante comunica√ß√£o</li>
                        <li class="cons">‚ùå <strong>Sem sobreposi√ß√£o</strong>: Comunica√ß√£o e computa√ß√£o n√£o podem ser simult√¢neas</li>
                    </ul>
                </div>

                <p><strong>Tempo de Execu√ß√£o</strong>: <code>T_total = T_comunica√ß√£o + T_computa√ß√£o</code></p>
                <p><strong>Melhor para</strong>: Poucos processos, comunica√ß√£o r√°pida, c√≥digos simples</p>
            </div>

            <div class="method-card">
                <h4>2. Comunica√ß√£o N√£o-Bloqueante: MPI_Isend/MPI_Irecv + MPI_Wait</h4>
                <p><strong>Defini√ß√£o</strong>: Opera√ß√µes de comunica√ß√£o <strong>ass√≠ncronas</strong> que iniciam imediatamente, mas requerem sincroniza√ß√£o expl√≠cita.</p>
                
                <pre><code>MPI_Request requests[4];
int req_count = 0;

// Iniciar comunica√ß√µes n√£o-bloqueantes
MPI_Isend(send_buffer, count, datatype, dest, tag, comm, &requests[req_count++]);
MPI_Irecv(recv_buffer, count, datatype, source, tag, comm, &requests[req_count++]);

// Fazer outras opera√ß√µes...

// Aguardar conclus√£o de TODAS as comunica√ß√µes
MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);</code></pre>

                <div class="pros-cons">
                    <p><strong>Caracter√≠sticas Te√≥ricas:</strong></p>
                    <ul>
                        <li class="pros">‚úÖ <strong>N√£o-bloqueante</strong>: Comunica√ß√£o inicia imediatamente</li>
                        <li class="pros">‚úÖ <strong>Flexibilidade</strong>: Permite m√∫ltiplas opera√ß√µes simult√¢neas</li>
                        <li class="pros">‚úÖ <strong>Menor lat√™ncia</strong>: Reduz tempo de espera entre processos</li>
                        <li class="cons">‚ùå <strong>Sincroniza√ß√£o obrigat√≥ria</strong>: MPI_Wait bloqueia at√© completar</li>
                        <li class="cons">‚ùå <strong>Limitada sobreposi√ß√£o</strong>: Computa√ß√£o s√≥ ap√≥s MPI_Waitall</li>
                    </ul>
                </div>

                <p><strong>Tempo de Execu√ß√£o</strong>: <code>T_total ‚âà max(T_setup_comunica√ß√£o, T_outras_ops) + T_computa√ß√£o_ap√≥s_wait</code></p>
                <p><strong>Melhor para</strong>: Cen√°rios com m√∫ltiplas comunica√ß√µes, quando h√° trabalho entre Isend/Irecv e Wait</p>
            </div>

            <div class="method-card">
                <h4>3. Comunica√ß√£o N√£o-Bloqueante: MPI_Isend/MPI_Irecv + MPI_Test</h4>
                <p><strong>Defini√ß√£o</strong>: Opera√ß√µes <strong>ass√≠ncronas</strong> com <strong>verifica√ß√£o peri√≥dica</strong> e <strong>m√°xima sobreposi√ß√£o</strong> computa√ß√£o/comunica√ß√£o.</p>
                
                <pre><code>// Iniciar comunica√ß√µes n√£o-bloqueantes
MPI_Isend/MPI_Irecv...

// PRIMEIRO: Computa√ß√£o que N√ÉO depende da comunica√ß√£o
for (int i = 2; i <= n_local-1; i++) {
    computacao_interna(i);
}

// Verificar comunica√ß√£o periodicamente
int all_complete = 0;
while (!all_complete) {
    MPI_Testall(req_count, requests, &all_complete, MPI_STATUSES_IGNORE);
    
    if (!all_complete) {
        // FAZER MAIS COMPUTA√á√ÉO enquanto aguarda
        outras_operacoes_uteis();
    }
}</code></pre>

                <div class="pros-cons">
                    <p><strong>Caracter√≠sticas Te√≥ricas:</strong></p>
                    <ul>
                        <li class="pros">‚úÖ <strong>M√°xima sobreposi√ß√£o</strong>: Comunica√ß√£o e computa√ß√£o verdadeiramente simult√¢neas</li>
                        <li class="pros">‚úÖ <strong>Efici√™ncia √≥tima</strong>: Aproveita 100% do tempo de CPU</li>
                        <li class="pros">‚úÖ <strong>Escalabilidade</strong>: Benef√≠cios crescem com mais processos</li>
                        <li class="pros">‚úÖ <strong>Flexibilidade total</strong>: Controle fino sobre quando verificar comunica√ß√£o</li>
                        <li class="cons">‚ùå <strong>Complexidade</strong>: C√≥digo mais elaborado para implementar</li>
                        <li class="cons">‚ùå <strong>Overhead do MPI_Test</strong>: Verifica√ß√µes frequentes consomem ciclos</li>
                    </ul>
                </div>

                <p><strong>Tempo de Execu√ß√£o</strong>: <code>T_total ‚âà max(T_comunica√ß√£o, T_computa√ß√£o_interna) + T_computa√ß√£o_bordas</code></p>
                <p><strong>Melhor para</strong>: Muitos processos, comunica√ß√£o lenta, problemas grandes com muito trabalho computacional</p>
            </div>
        </div>

        <h3>Compara√ß√£o Te√≥rica dos M√©todos</h3>
        <table>
            <thead>
                <tr>
                    <th>Aspecto</th>
                    <th><strong>Bloqueante</strong></th>
                    <th><strong>Wait</strong></th>
                    <th><strong>Test</strong></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Simplicidade</strong></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê</span></td>
                </tr>
                <tr>
                    <td><strong>Sobreposi√ß√£o</strong></td>
                    <td>‚ùå Nenhuma</td>
                    <td><span class="star-rating">‚≠ê‚≠ê</span> Limitada</td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</span> M√°xima</td>
                </tr>
                <tr>
                    <td><strong>Efici√™ncia CPU</strong></td>
                    <td><span class="star-rating">‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</span></td>
                </tr>
                <tr>
                    <td><strong>Overhead</strong></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</span> M√≠nimo</td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê</span> M√©dio</td>
                    <td><span class="star-rating">‚≠ê‚≠ê</span> Alto</td>
                </tr>
                <tr>
                    <td><strong>Escalabilidade</strong></td>
                    <td><span class="star-rating">‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê</span></td>
                    <td><span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</span></td>
                </tr>
            </tbody>
        </table>

        <h3>Expectativas Te√≥ricas de Performance</h3>
        <div class="performance-highlight">
            <p><strong>Para problemas pequenos</strong> (comunica√ß√£o >> computa√ß√£o):</p>
            <ul>
                <li><strong>Bloqueante</strong> deve ser melhor (menos overhead)</li>
                <li><strong>Wait</strong> ligeiramente pior (overhead sem benef√≠cio)</li>
                <li><strong>Test</strong> pior ainda (overhead alto, pouca sobreposi√ß√£o)</li>
            </ul>

            <p><strong>Para problemas grandes</strong> (computa√ß√£o >> comunica√ß√£o):</p>
            <ul>
                <li><strong>Test</strong> deve ser melhor (sobreposi√ß√£o efetiva)</li>
                <li><strong>Wait</strong> intermedi√°rio (alguma sobreposi√ß√£o)</li>
                <li><strong>Bloqueante</strong> pior (desperd√≠cio de CPU durante comunica√ß√£o)</li>
            </ul>

            <p><strong>Speedup esperado com problema adequado</strong>:</p>
            <ul>
                <li>Wait vs Bloqueante: <strong>1.1x - 1.3x</strong></li>
                <li>Test vs Bloqueante: <strong>1.2x - 2.0x</strong></li>
                <li>Test vs Wait: <strong>1.1x - 1.5x</strong></li>
            </ul>
        </div>

        <h2>Discretiza√ß√£o Num√©rica</h2>
        
        <h3>M√©todo de Diferen√ßas Finitas</h3>
        <p>A equa√ß√£o √© discretizada usando o <strong>m√©todo expl√≠cito de diferen√ßas finitas</strong>:</p>
        
        <div class="formula">
            T[i]<sup>(n+1)</sup> = T[i]<sup>n</sup> + Œ±*Œît/Œîx¬≤ * (T[i-1]<sup>n</sup> - 2*T[i]<sup>n</sup> + T[i+1]<sup>n</sup>)
        </div>

        <h3>Par√¢metros da Simula√ß√£o</h3>
        <ul>
            <li><strong>N_GLOBAL = 120000</strong>: N√∫mero total de pontos na barra</li>
            <li><strong>N_TIMESTEPS = 10000</strong>: N√∫mero de itera√ß√µes temporais</li>
            <li><strong>ALPHA = 0.1</strong>: Coeficiente de difus√£o t√©rmica</li>
            <li><strong>DT = 0.001</strong>: Passo temporal (Œît)</li>
            <li><strong>DX = 0.1</strong>: Espa√ßamento espacial (Œîx)</li>
        </ul>

        <h3>Condi√ß√µes Iniciais</h3>
        <ul>
            <li><strong>Pulso de calor</strong>: Temperatura de 100¬∞C no ter√ßo central da barra</li>
            <li><strong>Bordas frias</strong>: Temperatura de 0¬∞C no resto da barra</li>
            <li><strong>Evolu√ß√£o</strong>: O calor se difunde das regi√µes quentes para as frias</li>
        </ul>

        <h2>Paraleliza√ß√£o com MPI</h2>
        
        <h3>Decomposi√ß√£o de Dom√≠nio</h3>
        <p>A barra √© <strong>dividida entre os processos MPI</strong>:</p>
        <ul>
            <li>Cada processo simula <code>N_GLOBAL/size</code> pontos consecutivos</li>
            <li><strong>C√©lulas fantasma (ghost cells)</strong>: Cada processo mant√©m c√≥pias das bordas dos vizinhos</li>
            <li><strong>Comunica√ß√£o de bordas</strong>: Necess√°ria a cada itera√ß√£o temporal</li>
        </ul>

        <h3>Estrutura de Dados</h3>
        <pre><code>double *temp = calloc(n_local + 2, sizeof(double));
//                    |        |
//                    |        +-- C√©lulas fantasma (bordas dos vizinhos)
//                    +-- Pontos reais do processo</code></pre>

        <p><strong>Layout da mem√≥ria:</strong></p>
        <pre><code>[ghost_left] [dados_reais...] [ghost_right]
     ‚Üë              ‚Üë              ‚Üë
   temp[0]       temp[1..n]    temp[n+1]</code></pre>

        <h2>Tr√™s Implementa√ß√µes Comparadas</h2>

        <h3>1. Comunica√ß√£o Bloqueante (MPI_Send/MPI_Recv)</h3>
        <pre><code>// Trocar bordas com vizinhos
if (rank > 0) {
    MPI_Send(&temp[1], 1, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD);
    MPI_Recv(&temp[0], 1, MPI_DOUBLE, rank-1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}
if (rank < size-1) {
    MPI_Send(&temp[n_local], 1, MPI_DOUBLE, rank+1, 1, MPI_COMM_WORLD);
    MPI_Recv(&temp[n_local+1], 1, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}

// Atualizar todos os pontos ap√≥s comunica√ß√£o completar
for (int i = 1; i <= n_local; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}</code></pre>

        <p><strong>Caracter√≠sticas:</strong></p>
        <ul>
            <li class="pros">‚úÖ <strong>Simples de implementar</strong>: API direta e intuitiva</li>
            <li class="cons">‚ùå <strong>Serializa√ß√£o</strong>: Processos aguardam uns aos outros</li>
            <li class="cons">‚ùå <strong>Sem sobreposi√ß√£o</strong>: Comunica√ß√£o e computa√ß√£o n√£o podem ser simult√¢neas</li>
            <li>‚è±Ô∏è <strong>Tempo</strong>: <code>T_comunica√ß√£o + T_computa√ß√£o</code></li>
        </ul>

        <h3>2. Comunica√ß√£o N√£o-Bloqueante com MPI_Wait</h3>
        <pre><code>MPI_Request req[4];
int req_count = 0;

// Iniciar comunica√ß√µes n√£o-bloqueantes
if (rank > 0) {
    MPI_Isend(&temp[1], 1, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD, &req[req_count++]);
    MPI_Irecv(&temp[0], 1, MPI_DOUBLE, rank-1, 1, MPI_COMM_WORLD, &req[req_count++]);
}
if (rank < size-1) {
    MPI_Isend(&temp[n_local], 1, MPI_DOUBLE, rank+1, 1, MPI_COMM_WORLD, &req[req_count++]);
    MPI_Irecv(&temp[n_local+1], 1, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD, &req[req_count++]);
}

// Aguardar todas as comunica√ß√µes
MPI_Waitall(req_count, req, MPI_STATUSES_IGNORE);

// Computa√ß√£o ap√≥s comunica√ß√£o completar
for (int i = 1; i <= n_local; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}</code></pre>

        <p><strong>Caracter√≠sticas:</strong></p>
        <ul>
            <li class="pros">‚úÖ <strong>N√£o-bloqueante</strong>: Comunica√ß√£o inicia imediatamente</li>
            <li class="pros">‚úÖ <strong>Flexibilidade</strong>: Permite m√∫ltiplas opera√ß√µes simult√¢neas</li>
            <li class="cons">‚ùå <strong>Sem sobreposi√ß√£o</strong>: MPI_Waitall bloqueia at√© completar</li>
            <li>‚è±Ô∏è <strong>Tempo</strong>: Similar ao bloqueante, mas com menor overhead</li>
        </ul>

        <h3>3. Comunica√ß√£o N√£o-Bloqueante com MPI_Test (Sobreposi√ß√£o)</h3>
        <pre><code>// Iniciar comunica√ß√µes n√£o-bloqueantes
MPI_Isend/MPI_Irecv...

// PRIMEIRO: Atualizar pontos internos (n√£o precisam das bordas)
for (int i = 2; i <= n_local-1; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}

// Aguardar comunica√ß√µes usando MPI_Test em loop
int all_complete = 0;
while (!all_complete) {
    MPI_Testall(req_count, req, &flag, MPI_STATUSES_IGNORE);
    all_complete = flag;

    if (!all_complete) {
        // Fazer outras computa√ß√µes durante a espera
        volatile double dummy = 0.0;
        for (int k = 0; k < 100; k++) dummy += k * 0.001;
    }
}

// DEPOIS: Atualizar pontos das bordas (precisam dos valores dos vizinhos)
temp_new[1] = temp[1] + ALPHA * DT / (DX*DX) * (temp[0] - 2*temp[1] + temp[2]);
temp_new[n_local] = temp[n_local] + ALPHA * DT / (DX*DX) *
                   (temp[n_local-1] - 2*temp[n_local] + temp[n_local+1]);</code></pre>

        <p><strong>Caracter√≠sticas:</strong></p>
        <ul>
            <li class="pros">‚úÖ <strong>M√°xima sobreposi√ß√£o</strong>: Computa√ß√£o e comunica√ß√£o simult√¢neas</li>
            <li class="pros">‚úÖ <strong>Efici√™ncia</strong>: Aproveita tempo de espera da comunica√ß√£o</li>
            <li class="pros">‚úÖ <strong>Escalabilidade</strong>: Melhor para grandes n√∫meros de processos</li>
            <li>‚è±Ô∏è <strong>Tempo</strong>: <code>max(T_comunica√ß√£o, T_computa√ß√£o_interna) + T_computa√ß√£o_bordas</code></li>
        </ul>

        <h2>Padr√£o de Comunica√ß√£o</h2>
        
        <h3>Ghost Cells (C√©lulas Fantasma)</h3>
        <p>Cada processo precisa dos valores das <strong>bordas dos vizinhos</strong> para calcular a difus√£o:</p>
        
        <pre><code>Processo 0:    [x x x x] -> precisa do primeiro valor do Processo 1
Processo 1: <- [x x x x] -> precisa do √∫ltimo valor do Processo 0 e primeiro do Processo 2
Processo 2: <- [x x x x]    precisa do √∫ltimo valor do Processo 1</code></pre>

        <h3>Protocolo de Comunica√ß√£o</h3>
        <ol>
            <li><strong>Processo i envia</strong>:
                <ul>
                    <li>Primeira c√©lula para processo i-1 (se existe)</li>
                    <li>√öltima c√©lula para processo i+1 (se existe)</li>
                </ul>
            </li>
            <li><strong>Processo i recebe</strong>:
                <ul>
                    <li>√öltima c√©lula do processo i-1 na ghost cell esquerda</li>
                    <li>Primeira c√©lula do processo i+1 na ghost cell direita</li>
                </ul>
            </li>
        </ol>

        <h3>Tags das Mensagens</h3>
        <ul>
            <li><strong>Tag 0</strong>: Dados enviados para o vizinho da esquerda</li>
            <li><strong>Tag 1</strong>: Dados enviados para o vizinho da direita</li>
            <li><strong>Evita deadlocks</strong>: Cada send tem seu recv correspondente</li>
        </ul>

        <h2>Resultados Experimentais Obtidos (Compila√ß√£o sem Otimiza√ß√µes)</h2>
        <div class="results-section">
            
            <h3>Execu√ß√£o com 1 Processo</h3>
            <pre><code>====================================================
     SIMULACAO DE DIFUSAO DE CALOR 1D - MPI
====================================================
Tamanho da barra:      120000 pontos
Numero de processos:   1
Pontos por processo:   120000
Numero de iteracoes:   10000
Coef. difusao termica: 0.100
Passo temporal (dt):   0.001000
Espacamento (dx):      0.100
====================================================

RESULTADOS DE PERFORMANCE:
--------------------------------------------------
1. MPI_Send/MPI_Recv (bloqueante):              5.508369 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:              5.220360 s
3. MPI_Isend/MPI_Irecv + MPI_Test:              5.153071 s
--------------------------------------------------

PERFORMANCE (GFLOPS):
--------------------------------------------------
1. Comunicacao bloqueante:                          1.09 GFLOPS
2. Nao-bloqueante + Wait:                           1.15 GFLOPS
3. Nao-bloqueante + Test:                           1.16 GFLOPS
--------------------------------------------------

SPEEDUP RELATIVO:
--------------------------------------------------
Metodo 2 vs 1:                            1.06x
Metodo 3 vs 1:                            1.07x
Metodo 3 vs 2:                            1.01x
--------------------------------------------------</code></pre>

            <h3>Execu√ß√£o com 2 Processos</h3>
            <pre><code>====================================================
     SIMULACAO DE DIFUSAO DE CALOR 1D - MPI
====================================================
Tamanho da barra:      120000 pontos
Numero de processos:   2
Pontos por processo:   60000
Numero de iteracoes:   10000
Coef. difusao termica: 0.100
Passo temporal (dt):   0.001000
Espacamento (dx):      0.100
====================================================

RESULTADOS DE PERFORMANCE:
--------------------------------------------------
1. MPI_Send/MPI_Recv (bloqueante):              2.893398 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:              2.754426 s
3. MPI_Isend/MPI_Irecv + MPI_Test:              2.699148 s
--------------------------------------------------

PERFORMANCE (GFLOPS):
--------------------------------------------------
1. Comunicacao bloqueante:                          2.07 GFLOPS
2. Nao-bloqueante + Wait:                           2.18 GFLOPS
3. Nao-bloqueante + Test:                           2.22 GFLOPS
--------------------------------------------------

SPEEDUP RELATIVO:
--------------------------------------------------
Metodo 2 vs 1:                            1.05x
Metodo 3 vs 1:                            1.07x
Metodo 3 vs 2:                            1.02x
--------------------------------------------------</code></pre>

            <h3>Execu√ß√£o com 4 Processos</h3>
            <pre><code>====================================================
     SIMULACAO DE DIFUSAO DE CALOR 1D - MPI
====================================================
Tamanho da barra:      120000 pontos
Numero de processos:   4
Pontos por processo:   30000
Numero de iteracoes:   10000
Coef. difusao termica: 0.100
Passo temporal (dt):   0.001000
Espacamento (dx):      0.100
====================================================

RESULTADOS DE PERFORMANCE:
--------------------------------------------------
1. MPI_Send/MPI_Recv (bloqueante):              1.612336 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:              1.420571 s
3. MPI_Isend/MPI_Irecv + MPI_Test:              1.350074 s
--------------------------------------------------

PERFORMANCE (GFLOPS):
--------------------------------------------------
1. Comunicacao bloqueante:                          3.72 GFLOPS
2. Nao-bloqueante + Wait:                           4.22 GFLOPS
3. Nao-bloqueante + Test:                           4.44 GFLOPS
--------------------------------------------------

SPEEDUP RELATIVO:
--------------------------------------------------
Metodo 2 vs 1:                            1.13x
Metodo 3 vs 1:                            1.19x
Metodo 3 vs 2:                            1.05x
--------------------------------------------------</code></pre>


        </div>

        <h2>An√°lise Detalhada dos Resultados</h2>
        
        <h3>Compara√ß√£o: Resultados Esperados vs. Obtidos</h3>
        <p><strong>Resultados esperados (teoria):</strong></p>
        <ul>
            <li>Speedup vers√£o 2 vs 1: ~1.1x - 1.3x</li>
            <li>Speedup vers√£o 3 vs 1: ~1.2x - 2.0x</li>
            <li>Comunica√ß√£o n√£o-bloqueante sempre melhor</li>
        </ul>

        <p><strong>Resultados obtidos (compila√ß√£o sem otimiza√ß√µes -O0):</strong></p>
        <table>
            <thead>
                <tr>
                    <th>Processos</th>
                    <th>Melhor M√©todo</th>
                    <th>Tempo (s)</th>
                    <th>GFLOPS</th>
                    <th>Speedup v2/v1</th>
                    <th>Speedup v3/v1</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1</td>
                    <td class="highlight-best"><strong>Test</strong></td>
                    <td>5.153</td>
                    <td>1.16</td>
                    <td><strong>1.06x</strong> ‚úÖ</td>
                    <td><strong>1.07x</strong> ‚úÖ</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td class="highlight-best"><strong>Test</strong></td>
                    <td>2.699</td>
                    <td>2.22</td>
                    <td><strong>1.05x</strong> ‚úÖ</td>
                    <td><strong>1.07x</strong> ‚úÖ</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td class="highlight-best"><strong>Test</strong></td>
                    <td>1.350</td>
                    <td>4.44</td>
                    <td><strong>1.13x</strong> ‚úÖ</td>
                    <td><strong>1.19x</strong> ‚úÖ</td>
                </tr>
            </tbody>
        </table>

        <h3>An√°lise dos Resultados (Compila√ß√£o sem Otimiza√ß√µes)</h3>
        
        <h4>1. <strong>Confirma√ß√£o da Teoria MPI</strong></h4>
        <div class="performance-highlight">
            <p><strong>Com compila√ß√£o sem otimiza√ß√µes (-O0):</strong></p>
            <ul>
                <li>‚úÖ <strong>MPI_Test √© consistentemente melhor</strong> em todos os casos testados</li>
                <li>‚úÖ <strong>Speedups positivos</strong> conforme esperado na teoria</li>
                <li>‚úÖ <strong>Sobreposi√ß√£o computa√ß√£o/comunica√ß√£o</strong> ainda √© efetiva</li>
            </ul>

            <p><strong>Caracter√≠sticas observadas:</strong></p>
            <ul>
                <li><strong>Performance mais baixa</strong>: Sem otimiza√ß√µes, tempos s√£o mais altos</li>
                <li><strong>Speedups menores</strong>: 1.05x-1.19x (vs 1.16x-1.28x com -O2)</li>
                <li><strong>Mesma tend√™ncia</strong>: MPI_Test > MPI_Wait > Bloqueante</li>
            </ul>
        </div>

        <h4>2. <strong>Tend√™ncias Observadas</strong></h4>
        <p><strong>Escalabilidade clara:</strong></p>
        <ul>
            <li><strong>1 processo</strong>: Baseline sem comunica√ß√£o (1.16 GFLOPS)</li>
            <li><strong>2 processos</strong>: Speedup quase 2x no tempo total (2.22 GFLOPS)</li>
            <li><strong>4 processos</strong>: Melhor performance absoluta (4.44 GFLOPS)</li>
        </ul>

        <p><strong>Speedups sem otimiza√ß√µes:</strong></p>
        <ul>
            <li><strong>MPI_Wait vs Bloqueante</strong>: 1.05x - 1.13x ‚úÖ</li>
            <li><strong>MPI_Test vs Bloqueante</strong>: 1.07x - 1.19x ‚úÖ</li>
            <li><strong>Menores que com -O2</strong>: Mas ainda dentro da faixa te√≥rica</li>
        </ul>

        <h4>3. <strong>Impacto do Tamanho do Problema</strong></h4>
        <p><strong>Problema pequeno (4.8k pontos, 2k timesteps):</strong></p>
        <ul>
            <li>‚ùå <strong>Computa√ß√£o insuficiente</strong> para mascarar comunica√ß√£o</li>
            <li>‚ùå <strong>Overhead dominante</strong> sobre benef√≠cios</li>
            <li>‚ùå <strong>Tempos muito pequenos</strong> (< 0.1s) para medir diferen√ßas</li>
        </ul>

        <p><strong>Problema grande (120k pontos, 10k timesteps):</strong></p>
        <ul>
            <li>‚úÖ <strong>Computa√ß√£o suficiente</strong> para sobreposi√ß√£o efetiva</li>
            <li>‚úÖ <strong>Benef√≠cios superam overheads</strong> consistentemente</li>
            <li>‚úÖ <strong>Tempos mensur√°veis</strong> (1-3s) mostram diferen√ßas claras</li>
        </ul>

        <h2>Conclus√µes (Baseadas nos Resultados sem Otimiza√ß√µes)</h2>
        <div class="conclusion-box">
            <ol>
                <li><strong>MPI_Test √© consistentemente melhor</strong>: <strong>Superior em todos os casos testados (1, 2 e 4 processos)</strong></li>
                <li><strong>Speedups moderados mas consistentes</strong>: <strong>7-19% de melhoria sobre comunica√ß√£o bloqueante</strong></li>
                <li><strong>4 processos = ponto √≥timo</strong>: <strong>Melhor performance absoluta (4.44 GFLOPS)</strong></li>
                <li><strong>Teoria MPI confirmada</strong>: <strong>Resultados dentro da faixa te√≥rica esperada (1.1x-2.0x)</strong></li>
                <li><strong>Escalabilidade excelente</strong>: <strong>Performance quase dobra a cada duplica√ß√£o de processos</strong></li>
                <li><strong>Sobreposi√ß√£o funciona mesmo sem otimiza√ß√µes</strong>: <strong>Benef√≠cios persistem em c√≥digo n√£o otimizado</strong></li>
                <li><strong>Impacto das otimiza√ß√µes do compilador</strong>: <strong>C√≥digos sem -O2/-O3 t√™m speedups menores mas tend√™ncias iguais</strong></li>
                <li><strong>Comunica√ß√£o zero com 1 processo</strong>: <strong>Diferen√ßas m√≠nimas mostram overhead puro dos m√©todos MPI</strong></li>
            </ol>
        </div>

        <h2>Compila√ß√£o e Execu√ß√£o</h2>
        <div class="compilation-section">
            <h3>Compila√ß√£o</h3>
            <pre><code># Navegar para o diret√≥rio
cd /caminho/para/tarefa15/

# Compilar o programa (sem otimiza√ß√µes)
mpicc -o tarefa15 tarefa15.c -lm -Wall</code></pre>

            <h3>Execu√ß√£o</h3>
            <pre><code># Execu√ß√£o local com diferentes n√∫meros de processos
mpirun -np 2 ./tarefa15
mpirun -np 4 ./tarefa15
mpirun -np 8 ./tarefa15

# Execu√ß√£o com verbose (para debugging)
mpirun -np 4 --verbose ./tarefa15

# Execu√ß√£o em cluster (se dispon√≠vel)
mpirun -np 16 --hostfile hosts.txt ./tarefa15</code></pre>

            <h3><span class="emoji">üöÄ</span> <strong>Exemplo de Sa√≠da de Execu√ß√£o Completa (Compila√ß√£o sem Otimiza√ß√µes)</strong></h3>
            <pre><code>$ mpicc -o tarefa15 tarefa15.c -lm -Wall
$ mpirun -np 4 ./tarefa15

====================================================
     SIMULACAO DE DIFUSAO DE CALOR 1D - MPI
====================================================
Tamanho da barra:      120000 pontos
Numero de processos:   4
Pontos por processo:   30000
Numero de iteracoes:   10000
Coef. difusao termica: 0.100
Passo temporal (dt):   0.001000
Espacamento (dx):      0.100
====================================================

RESULTADOS DE PERFORMANCE:
--------------------------------------------------
1. MPI_Send/MPI_Recv (bloqueante):              1.612336 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:              1.420571 s
3. MPI_Isend/MPI_Irecv + MPI_Test:              1.350074 s
--------------------------------------------------

PERFORMANCE (GFLOPS):
--------------------------------------------------
1. Comunicacao bloqueante:                          3.72 GFLOPS
2. Nao-bloqueante + Wait:                           4.22 GFLOPS
3. Nao-bloqueante + Test:                           4.44 GFLOPS
--------------------------------------------------

SPEEDUP RELATIVO:
--------------------------------------------------
Metodo 2 vs 1:                            1.13x
Metodo 3 vs 1:                            1.19x
Metodo 3 vs 2:                            1.05x
--------------------------------------------------

ANALISE DE EFICIENCIA:
--------------------------------------------------
* MELHOR: Comunicacao nao-bloqueante + Test (1.350074 s)
  - Maxima flexibilidade de escalonamento
  - Ideal para sistemas heterogeneos
--------------------------------------------------
====================================================</code></pre>

            <h3><span class="emoji">üéØ</span> <strong>Dicas de Performance</strong></h3>
            <ol>
                <li><strong>N√∫mero de processos ideal</strong>: M√∫ltiplo do n√∫mero de cores do CPU</li>
                <li><strong>Memory binding</strong>: <code>mpirun --bind-to core -np 4 ./tarefa15</code></li>
                <li><strong>NUMA awareness</strong>: <code>mpirun --map-by numa -np 8 ./tarefa15</code></li>
                <li><strong>Profiling</strong>: Usar ferramentas como Intel VTune ou TAU</li>
            </ol>

            <h3><span class="emoji">üìä</span> <strong>Recomenda√ß√µes Baseadas nos Resultados</strong></h3>
            <p><strong>Para sistemas locais (baixa lat√™ncia):</strong></p>
            <ul>
                <li><strong>2 processos</strong>: Use comunica√ß√£o bloqueante (mais simples e eficiente)</li>
                <li><strong>4 processos</strong>: Use MPI_Wait (melhor performance absoluta: 3.81 GFLOPS)</li>
                <li><strong>8+ processos</strong>: Use MPI_Test se precisar de mais processos</li>
            </ul>

            <p><strong>Para clusters reais (alta lat√™ncia):</strong></p>
            <ul>
                <li><strong>Qualquer n√∫mero</strong>: Prefira MPI_Test (sobreposi√ß√£o se torna vantajosa)</li>
                <li><strong>Problemas ainda maiores</strong>: Aumente N_GLOBAL para 240000+ pontos</li>
            </ul>

            <p><strong>Regra geral obtida:</strong></p>
            <ul>
                <li><strong>Simplicidade primeiro</strong>: Use o m√©todo mais simples que atende sua performance</li>
                <li><strong>Me√ßa sempre</strong>: Resultados te√≥ricos podem diferir da pr√°tica</li>
                <li><strong>Contexto importa</strong>: Sistema, problema e n√∫mero de processos determinam a escolha √≥tima</li>
            </ul>
        </div>

        <div class="final-image">
            <h3>Resultados da Simula√ß√£o de Difus√£o de Calor</h3>
            <img src="tarefa15.png" alt="Gr√°fico dos resultados da simula√ß√£o de difus√£o de calor 1D com MPI mostrando a evolu√ß√£o temporal da temperatura ao longo da barra">
            <p><em>Figura: Visualiza√ß√£o da evolu√ß√£o temporal da difus√£o de calor na barra unidimensional, demonstrando como o pulso inicial de temperatura se propaga e se dissipa ao longo do tempo atrav√©s dos tr√™s m√©todos de comunica√ß√£o MPI implementados.</em></p>
        </div>
    </div>
</body>
</html>
