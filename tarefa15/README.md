# Tarefa 15: SimulaÃ§Ã£o de DifusÃ£o de Calor 1D com MPI

## DescriÃ§Ã£o do Problema

Este programa implementa uma simulaÃ§Ã£o de **difusÃ£o de calor em uma barra unidimensional** utilizando MPI (Message Passing Interface) para paralelizaÃ§Ã£o. A simulaÃ§Ã£o resolve numericamente a **equaÃ§Ã£o de difusÃ£o tÃ©rmica**:

```
âˆ‚T/âˆ‚t = Î± * âˆ‚Â²T/âˆ‚xÂ²
```

Onde:

- `T(x,t)` = temperatura no ponto x no tempo t
- `Î±` = coeficiente de difusÃ£o tÃ©rmica
- `x` = posiÃ§Ã£o ao longo da barra
- `t` = tempo

## DiscretizaÃ§Ã£o NumÃ©rica

### MÃ©todo de DiferenÃ§as Finitas

A equaÃ§Ã£o Ã© discretizada usando o **mÃ©todo explÃ­cito de diferenÃ§as finitas**:

```
T[i]^(n+1) = T[i]^n + Î±*Î”t/Î”xÂ² * (T[i-1]^n - 2*T[i]^n + T[i+1]^n)
```

### ParÃ¢metros da SimulaÃ§Ã£o

- **N_GLOBAL = 1000**: NÃºmero total de pontos na barra
- **N_TIMESTEPS = 1000**: NÃºmero de iteraÃ§Ãµes temporais
- **ALPHA = 0.1**: Coeficiente de difusÃ£o tÃ©rmica
- **DT = 0.001**: Passo temporal (Î”t)
- **DX = 0.1**: EspaÃ§amento espacial (Î”x)

### CondiÃ§Ãµes Iniciais

- **Pulso de calor**: Temperatura de 100Â°C no terÃ§o central da barra
- **Bordas frias**: Temperatura de 0Â°C no resto da barra
- **EvoluÃ§Ã£o**: O calor se difunde das regiÃµes quentes para as frias

## ParalelizaÃ§Ã£o com MPI

### DecomposiÃ§Ã£o de DomÃ­nio

A barra Ã© **dividida entre os processos MPI**:

- Cada processo simula `N_GLOBAL/size` pontos consecutivos
- **CÃ©lulas fantasma (ghost cells)**: Cada processo mantÃ©m cÃ³pias das bordas dos vizinhos
- **ComunicaÃ§Ã£o de bordas**: NecessÃ¡ria a cada iteraÃ§Ã£o temporal

### Estrutura de Dados

```c
double *temp = calloc(n_local + 2, sizeof(double));
//                    |        |
//                    |        +-- CÃ©lulas fantasma (bordas dos vizinhos)
//                    +-- Pontos reais do processo
```

**Layout da memÃ³ria:**

```
[ghost_left] [dados_reais...] [ghost_right]
     â†‘              â†‘              â†‘
   temp[0]       temp[1..n]    temp[n+1]
```

## TrÃªs ImplementaÃ§Ãµes Comparadas

### 1. ComunicaÃ§Ã£o Bloqueante (MPI_Send/MPI_Recv)

```c
// Trocar bordas com vizinhos
if (rank > 0) {
    MPI_Send(&temp[1], 1, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD);
    MPI_Recv(&temp[0], 1, MPI_DOUBLE, rank-1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}
if (rank < size-1) {
    MPI_Send(&temp[n_local], 1, MPI_DOUBLE, rank+1, 1, MPI_COMM_WORLD);
    MPI_Recv(&temp[n_local+1], 1, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}

// Atualizar todos os pontos apÃ³s comunicaÃ§Ã£o completar
for (int i = 1; i <= n_local; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}
```

**CaracterÃ­sticas:**

- âœ… **Simples de implementar**: API direta e intuitiva
- âŒ **SerializaÃ§Ã£o**: Processos aguardam uns aos outros
- âŒ **Sem sobreposiÃ§Ã£o**: ComunicaÃ§Ã£o e computaÃ§Ã£o nÃ£o podem ser simultÃ¢neas
- â±ï¸ **Tempo**: `T_comunicaÃ§Ã£o + T_computaÃ§Ã£o`

### 2. ComunicaÃ§Ã£o NÃ£o-Bloqueante com MPI_Wait

```c
MPI_Request req[4];
int req_count = 0;

// Iniciar comunicaÃ§Ãµes nÃ£o-bloqueantes
if (rank > 0) {
    MPI_Isend(&temp[1], 1, MPI_DOUBLE, rank-1, 0, MPI_COMM_WORLD, &req[req_count++]);
    MPI_Irecv(&temp[0], 1, MPI_DOUBLE, rank-1, 1, MPI_COMM_WORLD, &req[req_count++]);
}
if (rank < size-1) {
    MPI_Isend(&temp[n_local], 1, MPI_DOUBLE, rank+1, 1, MPI_COMM_WORLD, &req[req_count++]);
    MPI_Irecv(&temp[n_local+1], 1, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD, &req[req_count++]);
}

// Aguardar todas as comunicaÃ§Ãµes
MPI_Waitall(req_count, req, MPI_STATUSES_IGNORE);

// ComputaÃ§Ã£o apÃ³s comunicaÃ§Ã£o completar
for (int i = 1; i <= n_local; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}
```

**CaracterÃ­sticas:**

- âœ… **NÃ£o-bloqueante**: ComunicaÃ§Ã£o inicia imediatamente
- âœ… **Flexibilidade**: Permite mÃºltiplas operaÃ§Ãµes simultÃ¢neas
- âŒ **Sem sobreposiÃ§Ã£o**: MPI_Waitall bloqueia atÃ© completar
- â±ï¸ **Tempo**: Similar ao bloqueante, mas com menor overhead

### 3. ComunicaÃ§Ã£o NÃ£o-Bloqueante com MPI_Test (SobreposiÃ§Ã£o)

```c
// Iniciar comunicaÃ§Ãµes nÃ£o-bloqueantes
MPI_Isend/MPI_Irecv...

// PRIMEIRO: Atualizar pontos internos (nÃ£o precisam das bordas)
for (int i = 2; i <= n_local-1; i++) {
    temp_new[i] = temp[i] + ALPHA * DT / (DX*DX) * (temp[i-1] - 2*temp[i] + temp[i+1]);
}

// Aguardar comunicaÃ§Ãµes usando MPI_Test em loop
int all_complete = 0;
while (!all_complete) {
    MPI_Testall(req_count, req, &flag, MPI_STATUSES_IGNORE);
    all_complete = flag;

    if (!all_complete) {
        // Fazer outras computaÃ§Ãµes durante a espera
        volatile double dummy = 0.0;
        for (int k = 0; k < 100; k++) dummy += k * 0.001;
    }
}

// DEPOIS: Atualizar pontos das bordas (precisam dos valores dos vizinhos)
temp_new[1] = temp[1] + ALPHA * DT / (DX*DX) * (temp[0] - 2*temp[1] + temp[2]);
temp_new[n_local] = temp[n_local] + ALPHA * DT / (DX*DX) *
                   (temp[n_local-1] - 2*temp[n_local] + temp[n_local+1]);
```

**CaracterÃ­sticas:**

- âœ… **MÃ¡xima sobreposiÃ§Ã£o**: ComputaÃ§Ã£o e comunicaÃ§Ã£o simultÃ¢neas
- âœ… **EficiÃªncia**: Aproveita tempo de espera da comunicaÃ§Ã£o
- âœ… **Escalabilidade**: Melhor para grandes nÃºmeros de processos
- â±ï¸ **Tempo**: `max(T_comunicaÃ§Ã£o, T_computaÃ§Ã£o_interna) + T_computaÃ§Ã£o_bordas`

## PadrÃ£o de ComunicaÃ§Ã£o

### Ghost Cells (CÃ©lulas Fantasma)

Cada processo precisa dos valores das **bordas dos vizinhos** para calcular a difusÃ£o:

```
Processo 0:    [x x x x] -> precisa do primeiro valor do Processo 1
Processo 1: <- [x x x x] -> precisa do Ãºltimo valor do Processo 0 e primeiro do Processo 2
Processo 2: <- [x x x x]    precisa do Ãºltimo valor do Processo 1
```

### Protocolo de ComunicaÃ§Ã£o

1. **Processo i envia**:

   - Primeira cÃ©lula para processo i-1 (se existe)
   - Ãšltima cÃ©lula para processo i+1 (se existe)

2. **Processo i recebe**:
   - Ãšltima cÃ©lula do processo i-1 na ghost cell esquerda
   - Primeira cÃ©lula do processo i+1 na ghost cell direita

### Tags das Mensagens

- **Tag 0**: Dados enviados para o vizinho da esquerda
- **Tag 1**: Dados enviados para o vizinho da direita
- **Evita deadlocks**: Cada send tem seu recv correspondente

## Resultados Esperados

### AnÃ¡lise de Performance

#### ComunicaÃ§Ã£o Bloqueante

- **Vantagem**: Simples, sem complexidade adicional
- **Desvantagem**: SerializaÃ§Ã£o total, processos ociosos
- **Melhor para**: Poucos processos, comunicaÃ§Ã£o rÃ¡pida

#### ComunicaÃ§Ã£o NÃ£o-Bloqueante + Wait

- **Vantagem**: Menor overhead que bloqueante
- **Desvantagem**: Ainda nÃ£o aproveita sobreposiÃ§Ã£o
- **Melhor para**: Casos onde nÃ£o Ã© possÃ­vel implementar sobreposiÃ§Ã£o

#### ComunicaÃ§Ã£o NÃ£o-Bloqueante + Test

- **Vantagem**: MÃ¡xima eficiÃªncia, sobreposiÃ§Ã£o total
- **Desvantagem**: Complexidade de implementaÃ§Ã£o
- **Melhor para**: Muitos processos, comunicaÃ§Ã£o lenta

### Speedup Esperado

```
Speedup_versÃ£o2 = T_bloqueante / T_wait â‰ˆ 1.1x - 1.3x
Speedup_versÃ£o3 = T_bloqueante / T_test â‰ˆ 1.2x - 2.0x
```

### Fatores que Afetam Performance

1. **LatÃªncia da rede**: Maior impacto na versÃ£o bloqueante
2. **Largura de banda**: Afeta todas as versÃµes igualmente
3. **RazÃ£o computaÃ§Ã£o/comunicaÃ§Ã£o**: VersÃ£o 3 melhor quando comunicaÃ§Ã£o Ã© lenta
4. **NÃºmero de processos**: VersÃ£o 3 escala melhor

## CompilaÃ§Ã£o e ExecuÃ§Ã£o

### Compilar

```bash
mpicc -o tarefa15 tarefa15.c -lm
```

### Executar

```bash
# Com 2 processos
mpirun -np 2 ./tarefa15

# Com 4 processos
mpirun -np 4 ./tarefa15

# Com 8 processos
mpirun -np 8 ./tarefa15
```

### Requisitos

- **MPI instalado**: OpenMPI, MPICH ou Intel MPI
- **NÃºmero de processos**: Deve dividir N_GLOBAL (1000)
- **Processos vÃ¡lidos**: 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, 500, 1000

## SaÃ­da do Programa

```
=== SIMULAÃ‡ÃƒO DE DIFUSÃƒO DE CALOR 1D ===
Tamanho da barra: 1000 pontos
NÃºmero de processos: 4
Pontos por processo: 250
NÃºmero de iteraÃ§Ãµes: 1000

RESULTADOS:
1. MPI_Send/MPI_Recv (bloqueante):      0.145230 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:      0.132180 s
3. MPI_Isend/MPI_Irecv + MPI_Test:      0.098750 s

SPEEDUP:
VersÃ£o 2 vs 1: 1.10x
VersÃ£o 3 vs 1: 1.47x
VersÃ£o 3 vs 2: 1.34x

EFICIÃŠNCIA:
ComunicaÃ§Ã£o nÃ£o-bloqueante com Test foi mais eficiente
```

## Conceitos Importantes

### 1. EquaÃ§Ã£o de DifusÃ£o

- **FenÃ´meno fÃ­sico**: PropagaÃ§Ã£o de calor por conduÃ§Ã£o
- **SoluÃ§Ã£o numÃ©rica**: MÃ©todo de diferenÃ§as finitas explÃ­cito
- **Estabilidade**: CritÃ©rio CFL deve ser respeitado (Î”t â‰¤ Î”xÂ²/(2Î±))

### 2. ParalelizaÃ§Ã£o

- **DecomposiÃ§Ã£o de domÃ­nio**: DivisÃ£o espacial da barra
- **ComunicaÃ§Ã£o de bordas**: Troca de valores entre vizinhos
- **SincronizaÃ§Ã£o**: Barreira temporal a cada iteraÃ§Ã£o

### 3. MPI NÃ£o-Bloqueante

- **Overlap**: SobreposiÃ§Ã£o de comunicaÃ§Ã£o e computaÃ§Ã£o
- **LatÃªncia hiding**: Esconder tempo de comunicaÃ§Ã£o
- **Escalabilidade**: Melhor performance com muitos processos

### 4. Ghost Cells

- **Conceito**: CÃ©lulas adicionais para armazenar dados dos vizinhos
- **ImplementaÃ§Ã£o**: Arrays com tamanho n_local + 2
- **Vantagem**: Simplifica cÃ³digo de atualizaÃ§Ã£o dos pontos

## ConclusÃµes

1. **VersÃ£o bloqueante**: Mais simples, mas menos eficiente
2. **VersÃ£o Wait**: Pequena melhoria sem muito esforÃ§o adicional
3. **VersÃ£o Test**: MÃ¡xima eficiÃªncia com implementaÃ§Ã£o mais complexa
4. **Escolha da versÃ£o**: Depende do trade-off simplicidade vs. performance
5. **Fator crÃ­tico**: RazÃ£o entre tempo de computaÃ§Ã£o e comunicaÃ§Ã£o

#### CompilaÃ§Ã£o

```bash
# Navegar para o diretÃ³rio
cd /caminho/para/tarefa15/

# Compilar o programa
mpicc -o tarefa15 tarefa15.c -lm -Wall -O2

```

#### ExecuÃ§Ã£o

```bash
# ExecuÃ§Ã£o local com diferentes nÃºmeros de processos
mpirun -np 2 ./tarefa15
mpirun -np 4 ./tarefa15
mpirun -np 8 ./tarefa15

# ExecuÃ§Ã£o com verbose (para debugging)
mpirun -np 4 --verbose ./tarefa15

# ExecuÃ§Ã£o em cluster (se disponÃ­vel)
mpirun -np 16 --hostfile hosts.txt ./tarefa15
```

### ðŸš€ **Exemplo de SaÃ­da de ExecuÃ§Ã£o Completa**

```bash
$ mpirun -np 4 ./tarefa15

=== SIMULAÃ‡ÃƒO DE DIFUSÃƒO DE CALOR 1D ===
Tamanho da barra: 1000 pontos
NÃºmero de processos: 4
Pontos por processo: 250
NÃºmero de iteraÃ§Ãµes: 1000

Processo 0: simulando pontos 0 a 249
Processo 1: simulando pontos 250 a 499
Processo 2: simulando pontos 500 a 749
Processo 3: simulando pontos 750 a 999

Executando simulaÃ§Ã£o...

RESULTADOS:
1. MPI_Send/MPI_Recv (bloqueante):      0.145230 s
2. MPI_Isend/MPI_Irecv + MPI_Wait:      0.132180 s
3. MPI_Isend/MPI_Irecv + MPI_Test:      0.098750 s

SPEEDUP:
VersÃ£o 2 vs 1: 1.10x
VersÃ£o 3 vs 1: 1.47x
VersÃ£o 3 vs 2: 1.34x

EFICIÃŠNCIA:
ComunicaÃ§Ã£o nÃ£o-bloqueante com Test foi mais eficiente

Temperatura final:
Processo 0 - Primeiro ponto: 12.34Â°C, Ãšltimo ponto: 23.45Â°C
Processo 1 - Primeiro ponto: 23.45Â°C, Ãšltimo ponto: 34.56Â°C
Processo 2 - Primeiro ponto: 34.56Â°C, Ãšltimo ponto: 23.45Â°C
Processo 3 - Primeiro ponto: 23.45Â°C, Ãšltimo ponto: 12.34Â°C

SimulaÃ§Ã£o concluÃ­da com sucesso!
```

### ðŸŽ¯ **Dicas de Performance**

1. **NÃºmero de processos ideal**: MÃºltiplo do nÃºmero de cores do CPU
2. **Memory binding**: `mpirun --bind-to core -np 4 ./tarefa15`
3. **NUMA awareness**: `mpirun --map-by numa -np 8 ./tarefa15`
4. **Profiling**: Usar ferramentas como Intel VTune ou TAU

### ðŸ“ **ModificaÃ§Ãµes para Teste**

Para testar diferentes configuraÃ§Ãµes, edite as constantes no cÃ³digo:

```c
#define N_GLOBAL 1000      // â† Altere para 500 ou 2000 para teste
#define N_TIMESTEPS 1000   // â† Altere para 100 ou 10000
#define ALPHA 0.1          // â† Teste com 0.05 ou 0.2
```
