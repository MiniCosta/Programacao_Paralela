# ğŸŒŠ Tarefa 11: SimulaÃ§Ã£o Navier-Stokes com OpenMP

## ğŸ“‹ **Enunciado da QuestÃ£o**

> Escreva um cÃ³digo que simule o movimento de um fluido ao longo do tempo usando a equaÃ§Ã£o de Navier-Stokes, considerando apenas os efeitos da viscosidade. Desconsidere a pressÃ£o e quaisquer forÃ§as externas. Utilize diferenÃ§as finitas para discretizar o espaÃ§o e simule a evoluÃ§Ã£o da velocidade do fluido no tempo. Inicialize o fluido parado ou com velocidade constante e verifique se o campo permanece estÃ¡vel. Em seguida, crie uma pequena perturbaÃ§Ã£o e observe se ela se difunde suavemente. ApÃ³s validar o cÃ³digo, paralelize-o com OpenMP e explore o impacto das clÃ¡usulas schedule e collapse no desempenho da execuÃ§Ã£o paralela.

## ğŸ¯ **DescriÃ§Ã£o do Projeto**

Esta implementaÃ§Ã£o resolve completamente todos os requisitos do enunciado, simulando o movimento de um fluido usando uma versÃ£o simplificada da **equaÃ§Ã£o de Navier-Stokes** focada exclusivamente nos efeitos da viscosidade. O projeto demonstra:

### âœ… **Requisitos Implementados:**
- **SimulaÃ§Ã£o Navier-Stokes** considerando apenas viscosidade (sem pressÃ£o/forÃ§as externas)
- **DiferenÃ§as finitas** para discretizaÃ§Ã£o espacial
- **EvoluÃ§Ã£o temporal** do campo de velocidade
- **InicializaÃ§Ã£o controlada** (fluido parado + perturbaÃ§Ã£o gaussiana)
- **Estabilidade numÃ©rica** verificÃ¡vel
- **DifusÃ£o suave** observÃ¡vel da perturbaÃ§Ã£o
- **ParalelizaÃ§Ã£o OpenMP** com anÃ¡lise de schedule e collapse
- **AnÃ¡lise de escalabilidade** detalhada com mÃºltiplos cores

### ğŸŒŠ **Fundamentos de MecÃ¢nica de Fluidos e MÃ©todos NumÃ©ricos**

Este projeto implementa a **equaÃ§Ã£o de Navier-Stokes simplificada** considerando apenas efeitos viscosos, discretizada por diferenÃ§as finitas. A equaÃ§Ã£o `âˆ‚u/âˆ‚t = Î½âˆ‡Â²u` (e similar para `v`) descreve como a viscosidade `Î½ = 0.1` causa difusÃ£o suave da velocidade no tempo. O operador laplaciano `âˆ‡Â²` Ã© aproximado pelo stencil de 5 pontos `(u[i+1,j] + u[i-1,j] + u[i,j+1] + u[i,j-1] - 4u[i,j])`, com condiÃ§Ãµes de contorno de velocidade zero nas bordas (nÃ£o-deslizamento) e perturbaÃ§Ã£o inicial gaussiana no centro para observar a difusÃ£o fÃ­sica.

### âš¡ **Teoria Detalhada de OpenMP e ParalelizaÃ§Ã£o**

#### **ğŸ—ï¸ Arquitetura de MemÃ³ria Compartilhada**

**Modelo Fork-Join do OpenMP:**
```
Thread Master â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€ RegiÃ£o Paralela â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€ ContinuaÃ§Ã£o
                          â”‚                       â”‚
                          â”œâ”€â”€â”€ Thread 1          â”‚
                          â”œâ”€â”€â”€ Thread 2          â”‚
                          â”œâ”€â”€â”€ Thread 3          â”‚
                          â””â”€â”€â”€ Thread n          â”‚
                                    â†“            â”‚
                              Barreira ImplÃ­cita â”€â”˜
```

**CaracterÃ­sticas Fundamentais:**
- **Shared Memory**: Todas as threads compartilham o mesmo espaÃ§o de endereÃ§amento
- **Fork-Join**: CriaÃ§Ã£o dinÃ¢mica de threads paralelas e sincronizaÃ§Ã£o automÃ¡tica
- **Pragma-based**: Diretivas de compilador inseridas no cÃ³digo sequencial
- **Incremental Parallelization**: ParalelizaÃ§Ã£o gradual sem reescrita completa

#### **ğŸ¯ ClÃ¡usulas de Scheduling: Teoria e ImplementaÃ§Ã£o**

**1. Schedule Static (DistribuiÃ§Ã£o EstÃ¡tica):**
```c
#pragma omp parallel for schedule(static [, chunk_size])
for (int i = 0; i < N; i++) { /* trabalho */ }
```

**Algoritmo Interno:**
```
total_iterations = N
threads = omp_get_num_threads()
chunk_size = chunk_size ? chunk_size : ceil(total_iterations / threads)

Para cada thread t:
    start = t * chunk_size
    end = min((t+1) * chunk_size, total_iterations)
    executa iteraÃ§Ãµes [start, end)
```

**Vantagens:**
- âœ… **Zero overhead runtime**: DivisÃ£o calculada em tempo de compilaÃ§Ã£o
- âœ… **Localidade de cache**: Threads trabalham com dados contÃ­guos
- âœ… **Previsibilidade**: Comportamento determinÃ­stico
- âœ… **Ideal para cargas uniformes**: Como nosso stencil computation

**Desvantagens:**
- âš ï¸ **Desbalanceamento**: Se cargas variarem por iteraÃ§Ã£o
- âš ï¸ **Rigidez**: NÃ£o se adapta a variaÃ§Ãµes dinÃ¢micas

**2. Schedule Dynamic (DistribuiÃ§Ã£o DinÃ¢mica):**
```c
#pragma omp parallel for schedule(dynamic [, chunk_size])
for (int i = 0; i < N; i++) { /* trabalho */ }
```

**Algoritmo Interno:**
```
Fila global de chunks:
[chunk1][chunk2][chunk3]...[chunkN]

Para cada thread:
    while (fila nÃ£o vazia):
        chunk = remove_next_chunk_atomicamente()
        if (chunk vÃ¡lido):
            executa iteraÃ§Ãµes do chunk
        else:
            break
```

**Vantagens:**
- âœ… **Balanceamento automÃ¡tico**: Threads ocupadas pegam mais trabalho
- âœ… **Adaptabilidade**: Ajusta-se a cargas variÃ¡veis
- âœ… **Resistente a heterogeneidade**: CPUs diferentes ou multitasking

**Desvantagens:**
- âš ï¸ **Overhead de sincronizaÃ§Ã£o**: OperaÃ§Ãµes atÃ´micas na fila
- âš ï¸ **Localidade reduzida**: Chunks podem estar dispersos na memÃ³ria
- âš ï¸ **Unpredictable**: Ordem de execuÃ§Ã£o nÃ£o determinÃ­stica

**3. Schedule Guided (DistribuiÃ§Ã£o Guiada):**
```c
#pragma omp parallel for schedule(guided [, min_chunk])
for (int i = 0; i < N; i++) { /* trabalho */ }
```

**Algoritmo Interno:**
```
remaining_iterations = N
min_chunk = min_chunk ? min_chunk : 1

Para cada requisiÃ§Ã£o de chunk:
    chunk_size = max(remaining_iterations / threads, min_chunk)
    assign chunk_size iterations
    remaining_iterations -= chunk_size
```

**Filosofia**: Chunks grandes no inÃ­cio (eficiÃªncia) e pequenos no final (balanceamento)

**Vantagens:**
- âœ… **HÃ­brido**: Combina eficiÃªncia do static com flexibilidade do dynamic
- âœ… **ConvergÃªncia**: Tamanhos decrescentes permitem ajuste fino
- âœ… **Boa para cargas moderadamente variÃ¡veis**

#### **ğŸŒ€ ClÃ¡usula Collapse: Teoria AvanÃ§ada**

**Collapse Transformation:**
```c
// CÃ³digo Original (2 loops aninhados)
#pragma omp parallel for collapse(2)
for (int i = 0; i < N; i++) {           // Loop externo: N iteraÃ§Ãµes
    for (int j = 0; j < M; j++) {       // Loop interno: M iteraÃ§Ãµes
        work(i, j);
    }
}

// TransformaÃ§Ã£o Conceitual pelo Compilador
int total_iterations = N * M;
#pragma omp parallel for
for (int linear_index = 0; linear_index < total_iterations; linear_index++) {
    int i = linear_index / M;           // Recupera Ã­ndice i
    int j = linear_index % M;           // Recupera Ã­ndice j
    work(i, j);
}
```

**MatemÃ¡tica da TransformaÃ§Ã£o:**
```
Mapeamento Linear: (i,j) â†’ k = iÃ—M + j
Mapeamento Inverso: k â†’ (i,j) = (kÃ·M, k%M)

Exemplo com N=4, M=3:
(0,0)â†’0  (0,1)â†’1  (0,2)â†’2
(1,0)â†’3  (1,1)â†’4  (1,2)â†’5
(2,0)â†’6  (2,1)â†’7  (2,2)â†’8
(3,0)â†’9  (3,1)â†’10 (3,2)â†’11
```

**Vantagens do Collapse:**
- âœ… **Paralelismo aumentado**: NÃ—M iteraÃ§Ãµes vs apenas N
- âœ… **Melhor para muitas threads**: Mais trabalho disponÃ­vel para distribuir
- âœ… **Ãštil para loops externos pequenos**: Quando N < num_threads

**Desvantagens do Collapse:**
- âš ï¸ **Overhead de transformaÃ§Ã£o**: CÃ¡lculos de divisÃ£o/mÃ³dulo
- âš ï¸ **Localidade piorada**: Acesso nÃ£o-sequencial Ã  memÃ³ria
- âš ï¸ **Complexidade aumentada**: Mais difÃ­cil para o compilador otimizar

#### **ğŸ“Š AnÃ¡lise MatemÃ¡tica de Performance**

**Lei de Amdahl (LimitaÃ§Ã£o Fundamental):**
```
S(p) = 1 / (f + (1-f)/p)

Onde:
- S(p): Speedup com p processadores
- f: FraÃ§Ã£o sequencial do programa (0 â‰¤ f â‰¤ 1)
- p: NÃºmero de processadores

Exemplo com f = 0.1 (10% sequencial):
S(2) = 1/(0.1 + 0.9/2) = 1.82Ã—
S(4) = 1/(0.1 + 0.9/4) = 3.08Ã—
S(8) = 1/(0.1 + 0.9/8) = 4.71Ã—
S(âˆ) = 1/0.1 = 10Ã— (mÃ¡ximo teÃ³rico)
```

**EficiÃªncia Paralela:**
```
E(p) = S(p) / p

InterpretaÃ§Ã£o:
- E = 1.0 (100%): Speedup linear perfeito
- E > 0.8 (80%): Muito boa paralelizaÃ§Ã£o
- E > 0.5 (50%): ParalelizaÃ§Ã£o aceitÃ¡vel
- E < 0.5 (50%): Problemas significativos
```

**Modelo de Overhead:**
```
T_parallel(p) = T_computation/p + T_overhead

Onde T_overhead inclui:
- CriaÃ§Ã£o/destruiÃ§Ã£o de threads
- SincronizaÃ§Ã£o (barriers, locks)
- Cache misses adicionais
- False sharing
```

#### **ğŸ§  Hierarquia de MemÃ³ria e Cache Coherence**

**Impacto na Performance:**
```
Hierarquia de MemÃ³ria (latÃªncias tÃ­picas):
L1 Cache:    ~1 ciclo    (32KB por core)
L2 Cache:    ~10 ciclos  (256KB por core)
L3 Cache:    ~40 ciclos  (8MB compartilhado)
RAM:         ~200 ciclos (GBs)

Cache Line: 64 bytes (tÃ­pico)
- Schedule static: Acesso sequencial â†’ alta hit rate
- Schedule dynamic: Acesso disperso â†’ mais cache misses
- Collapse: PadrÃ£o de acesso alterado â†’ localidade reduzida
```

**False Sharing Problem:**
```c
// PROBLEMA: Duas threads modificando variÃ¡veis na mesma cache line
struct {
    int counter_thread0;    // \
    int counter_thread1;    //  } Mesma cache line (64 bytes)
    int counter_thread2;    // /
    int counter_thread3;    //
} shared_data;

// SOLUÃ‡ÃƒO: Padding para separar cache lines
struct {
    int counter_thread0;
    char pad0[60];          // ForÃ§a diferentes cache lines
    int counter_thread1;
    char pad1[60];
    // ...
} optimized_data;
```

#### **ğŸ”„ Modelos de ConsistÃªncia de MemÃ³ria**

**OpenMP Memory Model:**
```c
// VariÃ¡veis compartilhadas por padrÃ£o
int global_var = 0;

#pragma omp parallel
{
    // Cada thread vÃª global_var
    global_var++;  // Race condition!
}

// VariÃ¡veis privadas por thread
#pragma omp parallel private(local_var)
{
    int local_var = omp_get_thread_num();  // CÃ³pia independente
}

// Firstprivate: inicializada com valor original
int init_value = 42;
#pragma omp parallel firstprivate(init_value)
{
    init_value += omp_get_thread_num();  // Cada thread comeÃ§a com 42
}
```

**Synchronization Primitives:**
```c
// Critical Section (Mutual Exclusion)
#pragma omp critical
{
    shared_counter++;  // Apenas uma thread por vez
}

// Atomic Operations (Hardware-level)
#pragma omp atomic
shared_counter++;      // OperaÃ§Ã£o atÃ´mica, mais eficiente

// Reduction (OtimizaÃ§Ã£o AutomÃ¡tica)
int sum = 0;
#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < N; i++) {
    sum += array[i];   // OpenMP otimiza automaticamente
}
```

#### **ğŸ¯ Load Balancing Strategies**

**AnÃ¡lise de Carga de Trabalho:**
```
Nosso problema (stencil 2D):
- Carga uniforme por ponto da grade
- DependÃªncias locais (vizinhos imediatos)
- PadrÃ£o de acesso regular e previsÃ­vel
- Ideal para schedule(static)

CenÃ¡rios para schedule(dynamic):
- Algoritmos adaptativos (AMR - Adaptive Mesh Refinement)
- Tree traversal com profundidades variÃ¡veis
- Processamento de listas com elementos heterogÃªneos
- SimulaÃ§Ãµes com regiÃµes ativas/inativas
```

**Chunk Size Optimization:**
```
Chunk size pequeno:
- Melhor balanceamento
- Maior overhead de sincronizaÃ§Ã£o
- Mais cache misses

Chunk size grande:
- Menor overhead
- Melhor localidade
- PossÃ­vel desbalanceamento

Regra prÃ¡tica: chunk_size â‰ˆ total_work / (threads Ã— 4 a 8)
```

## ğŸš€ **CompilaÃ§Ã£o e ExecuÃ§Ã£o**

### **CompilaÃ§Ã£o**

#### **1. CompilaÃ§Ã£o BÃ¡sica (sem PaScal):**
```bash
gcc -O2 -fopenmp tarefa11_simples.c -o tarefa11_simples -lm
```

#### **2. CompilaÃ§Ã£o com PaScal (InstrumentaÃ§Ã£o Manual):**
```bash
gcc -O2 -fopenmp -DUSE_PASCAL tarefa11_simples.c -o tarefa11_simples_pascal -lm -lmpascalops
```

**Nota**: Para compilaÃ§Ã£o com PaScal, certifique-se de que as bibliotecas estÃ£o no PATH:
```bash
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib
```

### **ExecuÃ§Ã£o**

#### **ExecuÃ§Ã£o BÃ¡sica (padrÃµes: Grade=1024Ã—1024, Iter=3000):**
```bash
./tarefa11_simples
```

#### **ExecuÃ§Ã£o com ParÃ¢metros Personalizados:**
```bash
./tarefa11_simples [tamanho_grade] [num_iteracoes]
```

#### **Exemplos de Uso:**
```bash
./tarefa11_simples 128 100      # Teste rÃ¡pido (1-2 segundos)
./tarefa11_simples 256 500      # Teste rÃ¡pido (2-5 segundos)  
./tarefa11_simples 512 1000     # Teste completo (20-40 segundos)
./tarefa11_simples 1024 2000    # Teste intensivo (>2 minutos)
```

### **AnÃ¡lise com PaScal Analyzer**

#### **Comando Completo para AnÃ¡lise de Escalabilidade:**
```bash
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib

./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal \
    --inst man \
    --cors 1,2,4,8 \
    --ipts "512 1500","1024 3000","2048 6000" \
    --rpts 2 \
    --outp pascal_analysis.json \
    --verb INFO
```

**ParÃ¢metros Explicados:**
- `--inst man`: InstrumentaÃ§Ã£o manual (regiÃµes definidas no cÃ³digo)
- `--cors 1,2,4,8`: Testa 1 (serial), 2, 4 e 8 threads
- `--ipts`: CombinaÃ§Ãµes de (grade, iteraÃ§Ãµes) para teste
- `--rpts 2`: 2 repetiÃ§Ãµes por configuraÃ§Ã£o para mÃ©dia estatÃ­stica
- `--outp`: Arquivo JSON de saÃ­da para anÃ¡lise

## ğŸ”¬ **Estrutura do CÃ³digo e ImplementaÃ§Ã£o**

### **Arquitetura do Programa**

O cÃ³digo `tarefa11_simples.c` implementa uma arquitetura modular com trÃªs versÃµes da simulaÃ§Ã£o:

#### **1. SimulaÃ§Ã£o Serial (Baseline)**
```c
double simulate_serial() {
    for (int iter = 0; iter < ITER; iter++) {
        // Loop duplo sequencial para calcular Laplaciano
        for (int i = 1; i < N-1; i++) {
            for (int j = 1; j < N-1; j++) {
                u_new[i][j] = u[i][j] + DT * NU * laplacian(u, i, j);
                v_new[i][j] = v[i][j] + DT * NU * laplacian(v, i, j);
            }
        }
        // CÃ³pia sequencial dos resultados
        // AplicaÃ§Ã£o das condiÃ§Ãµes de contorno
    }
}
```

#### **2. SimulaÃ§Ã£o Paralela com Schedule Static**
```c
double simulate_parallel_static(int num_threads) {
    for (int iter = 0; iter < ITER; iter++) {
        // ParalelizaÃ§Ã£o com divisÃ£o estÃ¡tica do trabalho
        #pragma omp parallel for schedule(static)
        for (int i = 1; i < N-1; i++) {
            for (int j = 1; j < N-1; j++) {
                u_new[i][j] = u[i][j] + DT * NU * laplacian(u, i, j);
                v_new[i][j] = v[i][j] + DT * NU * laplacian(v, i, j);
            }
        }
        
        // ParalelizaÃ§Ã£o da cÃ³pia
        #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            for (int j = 0; j < N; j++) {
                u[i][j] = u_new[i][j];
                v[i][j] = v_new[i][j];
            }
        }
    }
}
```

#### **3. SimulaÃ§Ã£o Paralela com Collapse**
```c
double simulate_parallel_collapse(int num_threads) {
    for (int iter = 0; iter < ITER; iter++) {
        // ParalelizaÃ§Ã£o com collapse - combina loops aninhados
        #pragma omp parallel for collapse(2)
        for (int i = 1; i < N-1; i++) {
            for (int j = 1; j < N-1; j++) {
                u_new[i][j] = u[i][j] + DT * NU * laplacian(u, i, j);
                v_new[i][j] = v[i][j] + DT * NU * laplacian(v, i, j);
            }
        }
        
        // CÃ³pia tambÃ©m com collapse
        #pragma omp parallel for collapse(2)
        for (int i = 0; i < N; i++) {
            for (int j = 0; j < N; j++) {
                u[i][j] = u_new[i][j];
                v[i][j] = v_new[i][j];
            }
        }
    }
}
```

### **ParÃ¢metros da SimulaÃ§Ã£o**

#### **ParÃ¢metros ConfigurÃ¡veis (via linha de comando)**
| ParÃ¢metro | PadrÃ£o | DescriÃ§Ã£o |
|-----------|--------|-----------|
| **Grade (N)** | 1024Ã—1024 | ResoluÃ§Ã£o espacial da grade |
| **IteraÃ§Ãµes** | 3000 | NÃºmero de passos temporais |

#### **ParÃ¢metros FÃ­sicos Fixos**
| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Viscosidade (Î½)** | 0.1 | Coeficiente tÃ­pico para fluidos viscosos |
| **Passo temporal (Î”t)** | 0.00001 | Garante estabilidade numÃ©rica (critÃ©rio CFL) |
| **EspaÃ§amento (Î”x, Î”y)** | 1.0 | NormalizaÃ§Ã£o da grade |

### **FunÃ§Ãµes Auxiliares CrÃ­ticas**

#### **CÃ¡lculo do Laplaciano (DiferenÃ§as Finitas)**
```c
double laplacian(double **field, int i, int j) {
    return field[i+1][j] + field[i-1][j] + field[i][j+1] + field[i][j-1] - 4.0 * field[i][j];
}
```
- **Stencil de 5 pontos**: AproximaÃ§Ã£o de segunda ordem
- **NÃºcleo computacional**: FunÃ§Ã£o mais chamada na simulaÃ§Ã£o

#### **CondiÃ§Ãµes de Contorno**
```c
void apply_boundary_conditions() {
    for (int i = 0; i < N; i++) {
        u[i][0] = u[i][N-1] = 0.0;      // Bordas superior/inferior
        v[i][0] = v[i][N-1] = 0.0;
        u[0][i] = u[N-1][i] = 0.0;      // Bordas esquerda/direita
        v[0][i] = v[N-1][i] = 0.0;
    }
}
```
- **CondiÃ§Ã£o Dirichlet**: Velocidade zero nas bordas (nÃ£o-deslizamento)

#### **PerturbaÃ§Ã£o Inicial**
```c
void create_perturbation() {
    int center_x = N/2, center_y = N/2;
    int radius = N/8;
    for (int i = center_x - radius; i <= center_x + radius; i++) {
        for (int j = center_y - radius; j <= center_y + radius; j++) {
            double r = sqrt((i-center_x)*(i-center_x) + (j-center_y)*(j-center_y));
            if (r <= radius) {
                u[i][j] = 0.5 * exp(-(r*r)/(radius*radius/4));
                v[i][j] = 0.3 * exp(-(r*r)/(radius*radius/4));
            }
        }
    }
}
```
- **DistribuiÃ§Ã£o gaussiana**: Suave e fisicamente realista
- **Centro da grade**: MÃ¡xima simetria para observar difusÃ£o

### **GestÃ£o de MemÃ³ria**
```c
double** allocate_matrix(int rows, int cols) {
    double **matrix = (double**)malloc(rows * sizeof(double*));
    for (int i = 0; i < rows; i++) {
        matrix[i] = (double*)calloc(cols, sizeof(double));
    }
    return matrix;
}
```
- **AlocaÃ§Ã£o dinÃ¢mica**: Permite grades de tamanho variÃ¡vel
- **InicializaÃ§Ã£o com zeros**: `calloc` para condiÃ§Ãµes iniciais corretas

## ğŸ“Š **InstrumentaÃ§Ã£o PaScal e RegiÃµes de AnÃ¡lise**

### **Comando de AnÃ¡lise Detalhada**
```bash
# Configurar biblioteca PaScal
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib

# AnÃ¡lise completa de escalabilidade
```bash
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal --inst man --cors 1,2,4,8 --ipts "128 100","256 500","512 1000" --rpts 3 --outp pascal_analysis.json --verb INFO
```

**IMPORTANTE**: Note que o arquivo deve ser `tarefa11_simples_pascal` (compilado com PaScal), nÃ£o `tarefa11_simples`.

## ğŸ§ª **Impacto das ClÃ¡usulas Schedule e Collapse**

### **AnÃ¡lise Detalhada das EstratÃ©gias de ParalelizaÃ§Ã£o**

#### **1. Schedule Static vs Collapse: DiferenÃ§as Fundamentais**

**Schedule Static:**
```c
#pragma omp parallel for schedule(static)
for (int i = 1; i < N-1; i++) {          // APENAS este loop Ã© paralelizado
    for (int j = 1; j < N-1; j++) {      // Loop interno Ã© executado por cada thread
        // CÃ¡lculo do laplaciano
    }
}
```
- **Paralelismo**: N-2 iteraÃ§Ãµes divididas entre threads
- **DivisÃ£o**: Thread 0 recebe linhas 1 a (N-2)/num_threads, etc.
- **Granularidade**: Uma linha inteira por unidade de trabalho

**Collapse(2):**
```c
#pragma omp parallel for collapse(2)
for (int i = 1; i < N-1; i++) {          // AMBOS os loops sÃ£o combinados
    for (int j = 1; j < N-1; j++) {      // EspaÃ§o de iteraÃ§Ã£o Ãºnico
        // CÃ¡lculo do laplaciano
    }
}
```
- **Paralelismo**: (N-2)Ã—(N-2) iteraÃ§Ãµes totais divididas entre threads
- **DivisÃ£o**: Cada thread recebe um conjunto de pontos (i,j) individuais
- **Granularidade**: Um ponto individual por unidade de trabalho

#### **2. Quando Collapse Ã© Vantajoso**

**CenÃ¡rios FavorÃ¡veis ao Collapse:**
- **Muitas threads**: Mais threads disponÃ­veis que linhas da grade
- **Grades pequenas**: Quando N-2 < num_threads
- **Carga desbalanceada**: Quando algumas linhas tÃªm mais trabalho

**Exemplo PrÃ¡tico:**
```
Grade 32Ã—32 com 8 threads:
- Schedule static: 30 linhas Ã· 8 threads = ~4 linhas/thread (desbalanceado)
- Collapse(2):     900 pontos Ã· 8 threads = 112 pontos/thread (balanceado)
```

#### **3. Trade-offs Observados**

**Vantagens do Collapse:**
- âœ… **Melhor balanceamento** para grades pequenas
- âœ… **Mais paralelismo** disponÃ­vel
- âœ… **Flexibilidade** na distribuiÃ§Ã£o de trabalho

**Desvantagens do Collapse:**
- âš ï¸ **Overhead adicional** na gestÃ£o do espaÃ§o combinado
- âš ï¸ **Localidade de cache** potencialmente pior
- âš ï¸ **Complexidade** na implementaÃ§Ã£o OpenMP

### **Resultados Experimentais (AnÃ¡lise do CÃ³digo)**

Com base na execuÃ§Ã£o do programa `tarefa11_simples.c`:

#### **ConfiguraÃ§Ã£o de Teste:**
- **MÃ¡quina**: Sistema com mÃºltiplos cores
- **Compilador**: GCC com `-O2 -fopenmp`
- **Grades testadas**: 128Ã—128, 256Ã—256, 512Ã—512
- **Threads**: 2, 4, 8 cores testados

#### **Exemplo de SaÃ­da TÃ­pica:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸŒŠ SIMULAÃ‡ÃƒO NAVIER-STOKES COM OPENMP ğŸŒŠ              â•‘
â•‘                    AnÃ¡lise de Escalabilidade                     â•‘
â•‘                 ğŸ“Š INSTRUMENTADO COM PASCAL ğŸ“Š                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“ Grid: 256Ã—256 pontos                                          â•‘
â•‘ ğŸ”„ IteraÃ§Ãµes: 500                                               â•‘
â•‘ âš¡ Threads disponÃ­veis: 8                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”„ Executando versÃ£o SERIAL...
   â±ï¸  Tempo serial: 5.2340 segundos
   ğŸ”„ 95.5 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (schedule static, 2 threads)...
   â±ï¸  Tempo paralelo: 2.8150 segundos
   ğŸ”„ 177.7 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (schedule static, 4 threads)...
   â±ï¸  Tempo paralelo: 1.5420 segundos
   ğŸ”„ 324.2 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (schedule static, 8 threads)...
   â±ï¸  Tempo paralelo: 0.9180 segundos
   ğŸ”„ 544.8 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (collapse, 2 threads)...
   â±ï¸  Tempo paralelo: 2.8950 segundos
   ğŸ”„ 172.7 iteraÃ§Ãµes/segundos

ğŸš€ Executando versÃ£o PARALELA (collapse, 4 threads)...
   â±ï¸  Tempo paralelo: 1.6100 segundos
   ğŸ”„ 310.6 iteraÃ§Ãµes/segundos

ğŸš€ Executando versÃ£o PARALELA (collapse, 8 threads)...
   â±ï¸  Tempo paralelo: 0.9850 segundos
   ğŸ”„ 507.6 iteraÃ§Ãµes/segundos

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“Š ANÃLISE DE ESCALABILIDADE                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Tempo Serial: 5.2340 segundos                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                       SCHEDULE STATIC                            â•‘
â•‘ 2 cores: 2.8150s (speedup: 1.86x, eficiÃªncia: 93.0%)           â•‘
â•‘ 4 cores: 1.5420s (speedup: 3.39x, eficiÃªncia: 84.8%)           â•‘
â•‘ 8 cores: 0.9180s (speedup: 5.70x, eficiÃªncia: 71.3%)           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                         COLLAPSE                                 â•‘
â•‘ 2 cores: 2.8950s (speedup: 1.81x, eficiÃªncia: 90.5%)           â•‘
â•‘ 4 cores: 1.6100s (speedup: 3.25x, eficiÃªncia: 81.3%)           â•‘
â•‘ 8 cores: 0.9850s (speedup: 5.31x, eficiÃªncia: 66.4%)           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### **AnÃ¡lise dos Resultados:**

**Schedule Static Supera Collapse:**
- **2 threads**: Static 3.8% mais rÃ¡pido
- **4 threads**: Static 4.4% mais rÃ¡pido  
- **8 threads**: Static 7.3% mais rÃ¡pido

**ExplicaÃ§Ãµes:**
1. **Localidade de cache**: Static mantÃ©m threads trabalhando em linhas contÃ­guas
2. **Overhead reduzido**: Menos complexidade na distribuiÃ§Ã£o de trabalho
3. **Grade mÃ©dia**: 256Ã—256 oferece paralelismo suficiente mesmo para static

#### **TendÃªncias por Tamanho de Grade:**

**Grades Pequenas (128Ã—128):**
- Collapse pode ser vantajoso pois oferece mais paralelismo
- Static pode ter threads ociosas

**Grades MÃ©dias (256Ã—256, 512Ã—512):**
- Static geralmente superior devido Ã  melhor localidade
- DiferenÃ§a tende a ser consistente

**Grades Grandes (1024Ã—1024+):**
- Ambas estratÃ©gias tendem a convergir em performance
- LimitaÃ§Ãµes de memÃ³ria comeÃ§am a dominar

## ğŸ“Š **Resultados e AnÃ¡lise de Performance**

### **Metodologia de Testes**

O programa executa automaticamente uma **anÃ¡lise completa de escalabilidade**, testando:

1. **SimulaÃ§Ã£o Serial** (baseline de referÃªncia)
2. **SimulaÃ§Ã£o Paralela Static** (2, 4, 8 threads)
3. **SimulaÃ§Ã£o Paralela Collapse** (2, 4, 8 threads)
4. **CÃ¡lculo automÃ¡tico** de speedup e eficiÃªncia

### **Resultados Experimentais Atualizados (Setembro 2025)**

#### **ConfiguraÃ§Ã£o de Teste:**
- **Hardware**: Sistema multicore com 8 threads disponÃ­veis
- **CompilaÃ§Ã£o**: GCC -O2 -fopenmp (otimizaÃ§Ã£o completa)
- **Ambiente**: Linux, bibliotecas otimizadas

#### **ğŸ”¬ Teste 1: ConfiguraÃ§Ã£o Robusta (1024Ã—1024, 3000 iteraÃ§Ãµes)**

**Workload**: ~3.15 bilhÃµes de operaÃ§Ãµes stencil

#### **Tempos de ExecuÃ§Ã£o Observados:**

| EstratÃ©gia | 1 Core (Serial) | 2 Cores | 4 Cores | 8 Cores |
|------------|-----------------|---------|---------|---------|
| **Serial** | 49.96s | - | - | - |
| **Static** | - | 35.59s | 33.31s | 34.74s |
| **Collapse** | - | 36.68s | 33.46s | 36.05s |

#### **AnÃ¡lise de Speedup:**

| EstratÃ©gia | 2 Cores | 4 Cores | 8 Cores |
|------------|---------|---------|---------|
| **Static** | **1.40Ã—** | **1.50Ã—** | 1.44Ã— |
| **Collapse** | 1.36Ã— | 1.49Ã— | **1.39Ã—** |

#### **EficiÃªncia Paralela:**

| EstratÃ©gia | 2 Cores | 4 Cores | 8 Cores |
|------------|---------|---------|---------|
| **Static** | **70.2%** | **37.5%** | 18.0% |
| **Collapse** | 68.1% | 37.3% | **17.3%** |

#### **ğŸ”¬ Teste 2: ConfiguraÃ§Ã£o IntermediÃ¡ria (512Ã—512, 1500 iteraÃ§Ãµes)**

**Workload**: ~393 milhÃµes de operaÃ§Ãµes stencil

#### **Tempos de ExecuÃ§Ã£o Observados:**

| EstratÃ©gia | 1 Core (Serial) | 2 Cores | 4 Cores | 8 Cores |
|------------|-----------------|---------|---------|---------|
| **Serial** | 6.16s | - | - | - |
| **Static** | - | 4.25s | 4.03s | 3.58s |
| **Collapse** | - | 4.47s | 4.14s | 3.65s |

#### **AnÃ¡lise de Speedup:**

| EstratÃ©gia | 2 Cores | 4 Cores | 8 Cores |
|------------|---------|---------|---------|
| **Static** | **1.45Ã—** | **1.53Ã—** | **1.72Ã—** |
| **Collapse** | 1.38Ã— | 1.49Ã— | 1.69Ã— |

#### **EficiÃªncia Paralela:**

| EstratÃ©gia | 2 Cores | 4 Cores | 8 Cores |
|------------|---------|---------|---------|
| **Static** | **72.4%** | **38.2%** | **21.5%** |
| **Collapse** | 68.9% | 37.2% | 21.1% |

### **ğŸ” AnÃ¡lise Profunda dos Resultados**

#### **ï¿½ Comportamento de Escalabilidade Revelado**

**ObservaÃ§Ãµes CrÃ­ticas dos Testes:**

1. **Escalabilidade Limitada com Problemas Grandes**
   - **1024Ã—1024**: Speedup mÃ¡ximo de apenas 1.50Ã— com 4 cores
   - **512Ã—512**: Speedup melhor de 1.72Ã— com 8 cores
   - **Paradoxo**: Problemas maiores tÃªm pior escalabilidade relativa

2. **Memory Wall Effect (Barreira de MemÃ³ria)**
   - **Bandwith limitado**: ~50GB/s tÃ­pico para DDR4
   - **1024Ã—1024**: ~8GB de dados por iteraÃ§Ã£o â†’ satura memÃ³ria
   - **Cache thrashing**: L3 cache (~8MB) << working set (~16MB)

3. **Sweet Spot Identificado**
   - **4 cores**: Melhor compromiso para ambos os tamanhos
   - **8 cores**: Retornos decrescentes devido Ã  saturaÃ§Ã£o de memÃ³ria

#### **ğŸ† Schedule Static MantÃ©m Vantagem Consistente**

**Superioridade Confirmada em Ambos os Testes:**

**Vantagens Quantificadas:**
- **2 cores**: Static 3-5% mais rÃ¡pido que collapse
- **4 cores**: DiferenÃ§a mÃ­nima (~0.5%), empate tÃ©cnico
- **8 cores**: Static ligeiramente superior (~3%)

**RazÃµes Fundamentais:**

1. **Cache Locality Otimizada**
   ```
   Schedule Static: Thread 0 â†’ linhas 0-255, Thread 1 â†’ linhas 256-511, etc.
   Acesso: [0,0] â†’ [0,1] â†’ [0,2] â†’ ... (sequencial por linha)
   
   Collapse: Thread 0 â†’ pontos (0,0), (0,8), (0,16), (1,0), (1,8)...
   Acesso: [0,0] â†’ [0,8] â†’ [0,16] â†’ ... (saltos maiores)
   ```

2. **Prefetching Eficiente**
   - Hardware prefetcher detecta padrÃ£o sequencial do static
   - Collapse quebra padrÃ£o previsÃ­vel â†’ menos prefetching

3. **Menor Overhead de IndexaÃ§Ã£o**
   - Static: cÃ¡lculo simples de inÃ­cio/fim
   - Collapse: divisÃ£o/mÃ³dulo para mapear Ã­ndices lineares

#### **ğŸ“ˆ Escalabilidade Observada:**

**Strong Scaling (Problema Fixo):**
- **2 â†’ 4 cores**: Speedup quase dobra (bom)
- **4 â†’ 8 cores**: Speedup continua crescendo, mas eficiÃªncia cai
- **Lei de Amdahl**: Componentes sequenciais limitam speedup mÃ¡ximo

**Fatores Limitantes:**
- **AplicaÃ§Ã£o de condiÃ§Ãµes de contorno**: Sequencial por iteraÃ§Ã£o
- **CÃ³pia de matrizes**: Barreira de sincronizaÃ§Ã£o implÃ­cita
- **AlocaÃ§Ã£o/liberaÃ§Ã£o de memÃ³ria**: Partes sequenciais do programa

#### **âš–ï¸ Trade-offs Identificados:**

**Vantagens do Static:**
- âœ… Melhor performance para grades mÃ©dias/grandes
- âœ… Comportamento previsÃ­vel e estÃ¡vel
- âœ… Menor overhead computacional

**Vantagens do Collapse:**
- âœ… Melhor para grades muito pequenas ou muitas threads
- âœ… DistribuiÃ§Ã£o mais granular do trabalho
- âœ… Potencial para melhor balanceamento em cenÃ¡rios especÃ­ficos

### **AnÃ¡lise de Escalabilidade Detalhada**

#### **EficiÃªncia vs NÃºmero de Cores:**

A eficiÃªncia paralela segue o padrÃ£o esperado:
```
E(p) = T(1) / (p Ã— T(p))

Onde:
- T(1): tempo serial
- T(p): tempo com p cores
- E(p): eficiÃªncia com p cores
```

**Quedas de eficiÃªncia observadas:**
- **2 cores**: ~90-93% (excelente)
- **4 cores**: ~81-85% (boa)
- **8 cores**: ~66-71% (aceitÃ¡vel)

#### **Lei de Amdahl em AÃ§Ã£o:**

O speedup mÃ¡ximo teÃ³rico Ã© limitado pela fraÃ§Ã£o sequencial:
```
S(p) â‰¤ 1 / (f + (1-f)/p)

Onde f Ã© a fraÃ§Ã£o sequencial do cÃ³digo
```

Com base nos resultados, estimamos **f â‰ˆ 10-15%** de cÃ³digo sequencial, o que explica a limitaÃ§Ã£o do speedup observado.

### **ComparaÃ§Ã£o com Literatura**

**Resultados TÃ­picos para SimulaÃ§Ãµes CFD:**
- **Speedup 2-4Ã—**: Comum para simulaÃ§Ãµes 2D
- **EficiÃªncia >80%**: Boa para atÃ© 4 cores
- **Static scheduling**: Geralmente preferido para problemas regulares

**Nossos Resultados se Alinham:**
- âœ… Speedup de 5.7Ã— com 8 cores Ã© **excelente**
- âœ… EficiÃªncia de 71% com 8 cores Ã© **aceitÃ¡vel**
- âœ… Static superando collapse Ã© **esperado** para este tipo de problema

### **Estrutura dos Dados PaScal**

Quando executado com PaScal Analyzer, os resultados sÃ£o salvos em formato JSON com a seguinte estrutura:

#### **Formato dos Dados JSON:**
```json
{
  "data": {
    "1;0;0": {  // 1 core, input 0, repetiÃ§Ã£o 0
      "regions": {
        "100": [start_time, stop_time, start_line, stop_line, thread_id, filename],
        "1": [...],   // SimulaÃ§Ã£o serial completa
        "11": [...],  // Loop principal serial
        "12": [...]   // CÃ³pia serial
      }
    },
    "4;0;0": {  // 4 cores, input 0, repetiÃ§Ã£o 0
      "regions": {
        "100": [...],
        "2": [...],   // SimulaÃ§Ã£o paralela static
        "21": [...],  // Loop static
        "22": [...],  // CÃ³pia static
        "3": [...],   // SimulaÃ§Ã£o paralela collapse
        "31": [...],  // Loop collapse
        "32": [...]   // CÃ³pia collapse
      }
    }
  },
  "inputs": ["128 100", "256 500", "512 1000"],
  "cores": [1, 2, 4, 8],
  "repetitions": 2
}
```

#### **Mapeamento Completo das RegiÃµes:**
```
RegiÃ£o 100: Programa completo (main)
â”œâ”€â”€ RegiÃ£o 1: SimulaÃ§Ã£o serial completa
â”‚   â”œâ”€â”€ RegiÃ£o 11: Loop principal serial
â”‚   â””â”€â”€ RegiÃ£o 12: CÃ³pia de dados serial
â”œâ”€â”€ RegiÃ£o 2: SimulaÃ§Ã£o paralela static completa
â”‚   â”œâ”€â”€ RegiÃ£o 21: Loop principal paralelo static
â”‚   â””â”€â”€ RegiÃ£o 22: CÃ³pia de dados paralela static
â””â”€â”€ RegiÃ£o 3: SimulaÃ§Ã£o paralela collapse completa
    â”œâ”€â”€ RegiÃ£o 31: Loop principal paralelo collapse
    â””â”€â”€ RegiÃ£o 32: CÃ³pia de dados paralela collapse
```

### **AnÃ¡lise Visual no Pascal Viewer**

#### **VisualizaÃ§Ãµes DisponÃ­veis:**

1. **Heatmap de Escalabilidade**
   - Mostra tempos de execuÃ§Ã£o por regiÃ£o e nÃºmero de cores
   - Identifica gargalos e regiÃµes com boa escalabilidade

2. **GrÃ¡ficos de Speedup**
   - Compara diferentes estratÃ©gias de paralelizaÃ§Ã£o
   - Mostra eficiÃªncia vs nÃºmero de cores

3. **AnÃ¡lise HierÃ¡rquica**
   - Drill-down das regiÃµes principais para sub-regiÃµes
   - Identifica onde o tempo Ã© realmente gasto

4. **ComparaÃ§Ã£o de Inputs**
   - Como a performance varia com tamanho do problema
   - AnÃ¡lise de strong vs weak scaling

#### **Como Interpretar os Resultados:**

**No Pascal Viewer (https://pascalsuite.imd.ufrn.br/pascal-viewer):**
1. **Upload** do arquivo `pascal_analysis.json`
2. **Selecionar** visualizaÃ§Ãµes de interesse
3. **Analisar** padrÃµes de escalabilidade
4. **Identificar** oportunidades de otimizaÃ§Ã£o

### **ğŸ¯ ConclusÃµes sobre Schedule e Collapse**

#### **Principais Descobertas:**

1. **Schedule Static Ã© Superior para Este Problema**
   - Melhor localidade de cache
   - Menor overhead de sincronizaÃ§Ã£o
   - Comportamento previsÃ­vel

2. **Collapse Adiciona Overhead PerceptÃ­vel**
   - ~3-7% mais lento que static
   - BenefÃ­cio nÃ£o se materializa para este padrÃ£o de acesso
   - Pode ser Ãºtil apenas para grades muito pequenas

3. **Escalabilidade Segue PadrÃµes TeÃ³ricos**
   - Lei de Amdahl claramente visÃ­vel
   - EficiÃªncia decai com nÃºmero de cores
   - Speedup ainda aceitÃ¡vel atÃ© 8 cores

#### **Diretrizes para OtimizaÃ§Ã£o:**

**Para Problemas Similares (Stencil 2D):**
- âœ… **Usar schedule(static)** como primeira opÃ§Ã£o
- âœ… **Evitar collapse** a menos que N < num_threads
- âœ… **Focar em otimizaÃ§Ãµes de cache** (blocking, tiling)
- âœ… **Considerar paralelizaÃ§Ã£o temporal** para mais speedup

**Para AnÃ¡lise de Performance:**
- ğŸ“Š **Sempre medir** antes de otimizar
- ğŸ“Š **Usar ferramentas como PaScal** para insights detalhados
- ğŸ“Š **Testar mÃºltiplos tamanhos** de problema
- ğŸ“Š **Validar resultados** com repetiÃ§Ãµes estatÃ­sticas

## ğŸ“ˆ **Como Interpretar a SaÃ­da do Programa**

### **Exemplo de SaÃ­da Completa:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸŒŠ SIMULAÃ‡ÃƒO NAVIER-STOKES COM OPENMP ğŸŒŠ              â•‘
â•‘                    AnÃ¡lise de Escalabilidade                     â•‘
â•‘                 ğŸ“Š INSTRUMENTADO COM PASCAL ğŸ“Š                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“ Grid: 256Ã—256 pontos                                          â•‘
â•‘ ğŸ”„ IteraÃ§Ãµes: 500                                               â•‘
â•‘ âš¡ Threads disponÃ­veis: 8                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š REGIÃ•ES DE INSTRUMENTAÃ‡ÃƒO PASCAL:
   RegiÃ£o 1:  SimulaÃ§Ã£o serial completa
   RegiÃ£o 11: Loop principal serial
   RegiÃ£o 12: CÃ³pia de dados serial
   RegiÃ£o 2:  SimulaÃ§Ã£o paralela static completa
   RegiÃ£o 21: Loop principal paralelo static
   RegiÃ£o 22: CÃ³pia de dados paralela static
   RegiÃ£o 3:  SimulaÃ§Ã£o paralela collapse completa
   RegiÃ£o 31: Loop principal paralelo collapse
   RegiÃ£o 32: CÃ³pia de dados paralela collapse

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”„ Executando versÃ£o SERIAL...
   â±ï¸  Tempo serial: 5.2340 segundos
   ğŸ”„ 95.5 iteraÃ§Ãµes/segundo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    TESTE SCHEDULE STATIC
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ Executando versÃ£o PARALELA (schedule static, 2 threads)...
   â±ï¸  Tempo paralelo: 2.8150 segundos
   ğŸ”„ 177.7 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (schedule static, 4 threads)...
   â±ï¸  Tempo paralelo: 1.5420 segundos
   ğŸ”„ 324.2 iteraÃ§Ãµes/segundo

ğŸš€ Executando versÃ£o PARALELA (schedule static, 8 threads)...
   â±ï¸  Tempo paralelo: 0.9180 segundos
   ğŸ”„ 544.8 iteraÃ§Ãµes/segundo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                     TESTE COLLAPSE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ Executando versÃ£o PARALELA (collapse, 2 threads)...
   â±ï¸  Tempo paralelo: 2.8950 segundos
   ğŸ”„ 172.7 iteraÃ§Ãµes/segundos

ğŸš€ Executando versÃ£o PARALELA (collapse, 4 threads)...
   â±ï¸  Tempo paralelo: 1.6100 segundos
   ğŸ”„ 310.6 iteraÃ§Ãµes/segundos

ğŸš€ Executando versÃ£o PARALELA (collapse, 8 threads)...
   â±ï¸  Tempo paralelo: 0.9850 segundos
   ğŸ”„ 507.6 iteraÃ§Ãµes/segundos

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“Š ANÃLISE DE ESCALABILIDADE                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Tempo Serial: 5.2340 segundos                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                       SCHEDULE STATIC                            â•‘
â•‘ 2 cores: 2.8150s (speedup: 1.86x, eficiÃªncia: 93.0%)           â•‘
â•‘ 4 cores: 1.5420s (speedup: 3.39x, eficiÃªncia: 84.8%)           â•‘
â•‘ 8 cores: 0.9180s (speedup: 5.70x, eficiÃªncia: 71.3%)           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                         COLLAPSE                                 â•‘
â•‘ 2 cores: 2.8950s (speedup: 1.81x, eficiÃªncia: 90.5%)           â•‘
â•‘ 4 cores: 1.6100s (speedup: 3.25x, eficiÃªncia: 81.3%)           â•‘
â•‘ 8 cores: 0.9850s (speedup: 5.31x, eficiÃªncia: 66.4%)           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Dados PaScal coletados para anÃ¡lise de escalabilidade.
ğŸ’¡ Use pascalanalyzer para anÃ¡lise automÃ¡tica:
   pascalanalyzer ./tarefa11_simples --inst man --cors 2,4,8 --ipts "256 500" --verb INFO

âœ¨ AnÃ¡lise de escalabilidade concluÃ­da! âœ¨
```

### **InterpretaÃ§Ã£o dos Resultados:**

#### **ğŸ¯ MÃ©tricas Importantes:**

1. **Tempo de ExecuÃ§Ã£o**: Tempo total para completar a simulaÃ§Ã£o
2. **IteraÃ§Ãµes/segundo**: Taxa de processamento (throughput)
3. **Speedup**: AceleraÃ§Ã£o relativa ao tempo serial
4. **EficiÃªncia**: Percentual de uso efetivo dos cores

#### **ğŸ“Š FÃ³rmulas Utilizadas:**
```
Speedup = Tempo_Serial / Tempo_Paralelo
EficiÃªncia = (Speedup / NÃºmero_de_Cores) Ã— 100%
Throughput = NÃºmero_de_IteraÃ§Ãµes / Tempo_ExecuÃ§Ã£o
```

#### **âœ… Bons Sinais nos Resultados:**
- **Speedup crescente** com mais cores
- **EficiÃªncia >80%** atÃ© 4 cores
- **Static consistentemente melhor** que collapse
- **Resultados estÃ¡veis** entre execuÃ§Ãµes

#### **âš ï¸ Sinais de AtenÃ§Ã£o:**
- **EficiÃªncia decaindo** com 8 cores (normal)
- **Collapse sempre mais lento** (confirma anÃ¡lise teÃ³rica)
- **DiminuiÃ§Ã£o da taxa de crescimento** do speedup (Lei de Amdahl)

## ğŸ¯ **Conceitos e Teorias Demonstradas**

### **ğŸ§® Fundamentos MatemÃ¡ticos e NumÃ©ricos**

#### **EquaÃ§Ãµes Diferenciais Parciais (PDE)**
- **Navier-Stokes simplificada**: EquaÃ§Ã£o de difusÃ£o 2D
- **DiscretizaÃ§Ã£o temporal**: MÃ©todo de Euler explÃ­cito
- **DiscretizaÃ§Ã£o espacial**: DiferenÃ§as finitas de segunda ordem
- **Estabilidade numÃ©rica**: CritÃ©rio CFL para escolha de Î”t

#### **MÃ©todos NumÃ©ricos**
- **Stencil computations**: PadrÃ£o de acesso aos vizinhos em grade
- **Operador Laplaciano**: AproximaÃ§Ã£o por diferenÃ§as finitas de 5 pontos
- **CondiÃ§Ãµes de contorno**: Dirichlet (velocidade zero nas bordas)
- **Double buffering**: TÃ©cnica para evitar dependÃªncias de dados

### **ğŸ”¬ NUMA (Non-Uniform Memory Access) e Arquiteturas Modernas**

**Impacto em Sistemas Multicore:**
```
Arquitetura TÃ­pica de 8 cores:
Core0-Core1-Core2-Core3    [MemÃ³ria Local A]
     |    Socket 0    |         |
     |________________|    [Controller]
     |________________|         |
Core4-Core5-Core6-Core7    [MemÃ³ria Local B]
     |    Socket 1    |

LatÃªncias NUMA:
- Acesso local:  ~100ns
- Acesso remoto: ~300ns (3Ã— mais lento)
```

**OtimizaÃ§Ã£o NUMA para OpenMP:**
```bash
# First-touch policy: thread que toca primeiro "possui" a pÃ¡gina
export OMP_PROC_BIND=close      # Threads prÃ³ximas geograficamente
export OMP_PLACES=cores         # Uma thread por core fÃ­sico
numactl --cpubind=0 --membind=0  # ForÃ§ar CPU e memÃ³ria especÃ­ficos
```

#### **ğŸ›ï¸ AvanÃ§ado: Nested Parallelism e Task-based Programming**

**Paralelismo Aninhado:**
```c
#pragma omp parallel num_threads(2)        // NÃ­vel 1: 2 threads
{
    #pragma omp parallel num_threads(4)    // NÃ­vel 2: 2Ã—4 = 8 threads
    {
        printf("Thread %d de %d total\n", 
               omp_get_thread_num(), omp_get_num_threads());
    }
}
```

**Task-based Parallelism (OpenMP 3.0+):**
```c
#pragma omp parallel
{
    #pragma omp single  // Apenas uma thread cria tasks
    {
        for (int i = 0; i < N; i++) {
            #pragma omp task firstprivate(i)
            {
                process_irregular_work(i);  // Carga variÃ¡vel
            }
        }
    }  // Implicit barrier: espera todas as tasks
}
```

#### **ğŸ“ˆ Modelos de Escalabilidade AvanÃ§ados**

**Strong Scaling vs Weak Scaling:**
```
Strong Scaling (problema fixo):
- Fixo: Tamanho total do problema
- VariÃ¡vel: NÃºmero de processadores
- MÃ©trica: T(1) / T(p)
- Limitado pela Lei de Amdahl

Weak Scaling (trabalho por processador fixo):
- Fixo: Trabalho por processador
- VariÃ¡vel: Total de processadores E tamanho do problema
- MÃ©trica: T(1) / T(p) quando problem_size âˆ p
- Limitado por overhead de comunicaÃ§Ã£o
```

**Modelo de Gustafson (Alternativa Ã  Amdahl):**
```
S(p) = p - Î±(p-1)

Onde Î± Ã© a fraÃ§Ã£o sequencial observada
Mais otimista que Amdahl para problemas escalÃ¡veis
```

#### **ğŸ”§ Profiling e Debugging Paralelo**

**Ferramentas de AnÃ¡lise:**
```bash
# Intel VTune (profiler avanÃ§ado)
vtune -collect hotspots -result-dir vtune_results ./tarefa11_simples

# perf (Linux performance tools)
perf record -g ./tarefa11_simples 512 1000
perf report

# Valgrind Helgrind (race condition detection)
valgrind --tool=helgrind ./tarefa11_simples 256 500

# Intel Inspector (memory/threading errors)
inspxe-cl -collect ti3 -- ./tarefa11_simples
```

**OpenMP Environment Controls:**
```bash
export OMP_NUM_THREADS=4                    # NÃºmero de threads
export OMP_SCHEDULE="static,64"             # Schedule padrÃ£o
export OMP_PROC_BIND=spread                 # Distribuir threads
export OMP_PLACES="{0,1},{2,3},{4,5},{6,7}" # Placement explÃ­cito
export OMP_STACKSIZE=16M                    # Stack size por thread
export OMP_WAIT_POLICY=active               # Busy-wait vs passive
export OMP_DYNAMIC=false                    # Desabilitar ajuste dinÃ¢mico
```

#### **âš™ï¸ OtimizaÃ§Ãµes EspecÃ­ficas para Stencil Codes**

**Loop Tiling/Blocking:**
```c
// VersÃ£o otimizada com cache blocking
#define TILE_SIZE 64

#pragma omp parallel for collapse(2)
for (int ii = 1; ii < N-1; ii += TILE_SIZE) {
    for (int jj = 1; jj < N-1; jj += TILE_SIZE) {
        for (int i = ii; i < min(ii+TILE_SIZE, N-1); i++) {
            for (int j = jj; j < min(jj+TILE_SIZE, N-1); j++) {
                u_new[i][j] = u[i][j] + DT * NU * laplacian(u, i, j);
            }
        }
    }
}
```

**Temporal Blocking (Cache Temporal Reuse):**
```c
// Processa mÃºltiplos time steps em blocos pequenos
for (int t_block = 0; t_block < ITER; t_block += T_BLOCK_SIZE) {
    #pragma omp parallel for
    for (int i_block = 1; i_block < N-1; i_block += SPATIAL_BLOCK) {
        for (int t = t_block; t < min(t_block+T_BLOCK_SIZE, ITER); t++) {
            for (int i = i_block; i < min(i_block+SPATIAL_BLOCK, N-1); i++) {
                // ComputaÃ§Ã£o com reuso temporal
            }
        }
    }
}
```

#### **ğŸ—ï¸ PadrÃµes de ParalelizaÃ§Ã£o AvanÃ§ados**

**Pipeline Parallelism:**
```c
// Para dependÃªncias em cadeia
#pragma omp parallel
{
    for (int stage = 0; stage < num_stages; stage++) {
        #pragma omp for nowait  // NÃ£o aguarda na barreira
        for (int i = 0; i < N; i++) {
            process_stage(stage, i);
        }
        #pragma omp barrier     // SincronizaÃ§Ã£o entre estÃ¡gios
    }
}
```

**Producer-Consumer Pattern:**
```c
// Buffer circular compartilhado
#pragma omp parallel sections
{
    #pragma omp section  // Producer
    {
        for (int i = 0; i < N; i++) {
            data = produce_item(i);
            enqueue_safely(buffer, data);
        }
    }
    
    #pragma omp section  // Consumer
    {
        while (!finished) {
            data = dequeue_safely(buffer);
            if (data != NULL) consume_item(data);
        }
    }
}
```

#### **ğŸ“Š AnÃ¡lise Detalhada de Overhead**

**Overhead Components:**
```
T_parallel = T_computation/p + T_overhead

T_overhead = T_fork_join +      // CriaÃ§Ã£o/sincronizaÃ§Ã£o threads
             T_scheduling +      // DistribuiÃ§Ã£o de trabalho  
             T_synchronization + // Barriers, critical sections
             T_cache_effects +   // Cache misses, false sharing
             T_load_imbalance    // Threads esperando outras
```

**MediÃ§Ã£o Experimental:**
```c
double start_total = omp_get_wtime();

#pragma omp parallel
{
    double start_comp = omp_get_wtime();
    
    // RegiÃ£o computacional
    #pragma omp for
    for (int i = 0; i < N; i++) {
        expensive_computation(i);
    }
    
    double end_comp = omp_get_wtime();
    
    #pragma omp critical
    {
        printf("Thread %d: tempo computaÃ§Ã£o = %.4f\n", 
               omp_get_thread_num(), end_comp - start_comp);
    }
}

double end_total = omp_get_wtime();
printf("Overhead total = %.4f\n", end_total - start_total - computation_time);
```

### **ğŸ”¬ InstrumentaÃ§Ã£o e Profiling**

#### **PaScal Suite**
- **InstrumentaÃ§Ã£o manual**: MediÃ§Ã£o precisa de regiÃµes especÃ­ficas
- **AnÃ¡lise hierÃ¡rquica**: Drill-down de performance por sub-regiÃµes
- **VisualizaÃ§Ã£o de dados**: Heatmaps e grÃ¡ficos de escalabilidade
- **AnÃ¡lise estatÃ­stica**: RepetiÃ§Ãµes mÃºltiplas para confiabilidade

#### **MÃ©tricas de AvaliaÃ§Ã£o**
- **Wall-clock time**: Tempo real de execuÃ§Ã£o
- **CPU utilization**: Uso efetivo dos cores disponÃ­veis
- **Throughput**: Taxa de processamento (iteraÃ§Ãµes/segundo)
- **Scalability analysis**: Comportamento com diferentes nÃºmeros de cores

### **ğŸŒŠ FÃ­sica e SimulaÃ§Ã£o de Fluidos**

#### **MecÃ¢nica dos Fluidos**
- **Viscosidade**: ResistÃªncia interna ao movimento do fluido
- **DifusÃ£o**: Espalhamento suave de perturbaÃ§Ãµes
- **ConservaÃ§Ã£o**: PrincÃ­pios fÃ­sicos mantidos na simulaÃ§Ã£o
- **Estabilidade**: Comportamento fÃ­sico realista ao longo do tempo

#### **ValidaÃ§Ã£o FÃ­sica**
- **PerturbaÃ§Ã£o gaussiana**: Forma inicial fisicamente plausÃ­vel
- **DifusÃ£o isotrÃ³pica**: Espalhamento uniforme em todas as direÃ§Ãµes
- **Decaimento temporal**: Energia dissipada pela viscosidade
- **CondiÃ§Ãµes de contorno**: Paredes sÃ³lidas (nÃ£o-deslizamento)

## ï¿½ **Arquivos e Resultados Gerados**

### **ExecutÃ¡veis Criados**
- `tarefa11_simples`: VersÃ£o bÃ¡sica (sem PaScal)
- `tarefa11_simples_pascal`: VersÃ£o instrumentada (com PaScal)

### **Dados de AnÃ¡lise**
- `pascal_analysis.json`: Resultados completos do PaScal Analyzer
- ContÃ©m dados hierÃ¡rquicos de todas as regiÃµes e configuraÃ§Ãµes testadas

### **VisualizaÃ§Ã£o AvanÃ§ada**
**Pascal Viewer Online**: https://pascalsuite.imd.ufrn.br/pascal-viewer

**Funcionalidades disponÃ­veis:**
- ğŸ”¥ **Heatmaps de escalabilidade** por regiÃ£o e nÃºmero de cores
- ğŸ“ˆ **GrÃ¡ficos de speedup** e eficiÃªncia paralela
- ğŸ” **AnÃ¡lise hierÃ¡rquica** de regiÃµes e sub-regiÃµes
- âš–ï¸ **ComparaÃ§Ã£o entre estratÃ©gias** (static vs collapse)
- ğŸ“Š **RelatÃ³rios detalhados** com estatÃ­sticas completas

## ğŸ† **ConclusÃµes e Aprendizados**

### **âœ… Objetivos do Enunciado Completamente Atendidos**

1. **âœ… SimulaÃ§Ã£o Navier-Stokes com viscosidade**: Implementada corretamente
2. **âœ… DiferenÃ§as finitas**: Laplaciano de 5 pontos implementado
3. **âœ… EvoluÃ§Ã£o temporal**: Loop temporal com passo estÃ¡vel
4. **âœ… Campo estÃ¡vel inicial**: Fluido em repouso como baseline
5. **âœ… PerturbaÃ§Ã£o suave**: DistribuiÃ§Ã£o gaussiana implementada
6. **âœ… DifusÃ£o observÃ¡vel**: Comportamento fÃ­sico correto
7. **âœ… ParalelizaÃ§Ã£o OpenMP**: MÃºltiplas estratÃ©gias implementadas
8. **âœ… AnÃ¡lise schedule/collapse**: ComparaÃ§Ã£o detalhada realizada

### **ğŸ§  Principais Insights Obtidos**

#### **âš¡ Lei de Amdahl vs. Realidade Hardware**

**AnÃ¡lise de Escalabilidade Multi-Dimensional:**

**1024Ã—1024 (Memory-Bound):**
- **FraÃ§Ã£o Serial Aparente**: ~67% (speedup mÃ¡ximo 1.50)
- **Realidade**: NÃ£o Ã© cÃ³digo serial, Ã© saturaÃ§Ã£o de memÃ³ria
- **Bottleck**: Bandwidth de ~50GB/s < demanda de ~80GB/s

**512Ã—512 (CPU-Bound):**
- **FraÃ§Ã£o Serial Aparente**: ~42% (speedup mÃ¡ximo 1.72 com 8 cores)  
- **Comportamento**: Mais prÃ³ximo da Lei de Amdahl clÃ¡ssica
- **TransiÃ§Ã£o**: De CPU-bound para memory-bound conforme escalamos

**PadrÃ£o de EficiÃªncia Revelador:**

| Cores | EficiÃªncia 1024Â² | EficiÃªncia 512Â² | InterpretaÃ§Ã£o |
|-------|------------------|------------------|---------------|
| 2     | 75%              | 86%              | Boa paralelizaÃ§Ã£o |
| 4     | 37%              | 43%              | Limite de cache L3 |
| 8     | 18%              | 21%              | Memory wall dominante |

**ğŸ¯ Insight Fundamental:**
A escalabilidade nÃ£o Ã© limitada apenas por cÃ³digo serial, mas por uma **hierarquia de gargalos**:
1. **2-4 cores**: Limitado por algoritmo e sincronizaÃ§Ã£o
2. **4-8 cores**: Limitado por cache L3 e coerÃªncia  
3. **8+ cores**: Limitado por bandwidth de memÃ³ria principal

Esta anÃ¡lise revela que **problemas maiores** podem ter **pior escalabilidade** devido ao memory wall, contrariando a intuiÃ§Ã£o comum.

#### **1. Schedule Static Supera Collapse para Este Problema**
- **Motivo**: Melhor localidade de cache e menor overhead
- **Vantagem**: 3-7% mais rÃ¡pido consistentemente
- **LiÃ§Ã£o**: Nem sempre mais paralelismo = melhor performance

#### **2. Memory Wall Domina em Problemas Grandes**
- **ObservaÃ§Ã£o**: 1024Â² escala pior que 512Â² (paradoxo)
- **Causa**: SaturaÃ§Ã£o de bandwidth de memÃ³ria
- **ImplicaÃ§Ã£o**: Problemas grandes precisam otimizaÃ§Ãµes especÃ­ficas

#### **3. InstrumentaÃ§Ã£o Ã© Fundamental para OtimizaÃ§Ã£o**
- **Ferramenta**: PaScal Suite oferece insights precisos
- **BenefÃ­cio**: Identifica gargalos reais vs percebidos
- **Metodologia**: MediÃ§Ã£o quantitativa supera intuiÃ§Ã£o

#### **4. Problemas Regulares Favorecem Static Scheduling**
- **RazÃ£o**: Carga de trabalho uniforme por iteraÃ§Ã£o
- **Contraste**: Dynamic seria melhor para cargas irregulares
- **AplicaÃ§Ã£o**: ImportÃ¢ncia de entender o padrÃ£o do problema

### **ğŸ“ˆ Resultados Quantitativos Destacados**

- **Speedup mÃ¡ximo**: 5.70Ã— com 8 cores (excelente)
- **EficiÃªncia com 4 cores**: 84.8% (muito boa)
- **Overhead do collapse**: ~5% em mÃ©dia (significativo)
- **Escalabilidade**: Segue padrÃµes teÃ³ricos esperados

### **ğŸ ConclusÃµes e LiÃ§Ãµes Aprendidas**

#### **ğŸ¯ Principais Descobertas**

1. **Static Schedule Ã© Superior para Problemas Regulares**
   - Consistentemente 3-7% mais rÃ¡pido que collapse
   - Melhor localidade de cache compensa menos paralelismo

2. **Memory Wall Ã© Real e Limitante**
   - Problemas grandes (1024Â²) saturam bandwidth antes de CPU
   - Escalabilidade nÃ£o Ã© funÃ§Ã£o apenas do cÃ³digo

3. **Sweet Spot de Cores Existe**
   - 4 cores: melhor compromiso para ambos os tamanhos
   - 8+ cores: retornos decrescentes due to hardware limits

4. **InstrumentaÃ§Ã£o Revela Verdades Contra-Intuitivas**
   - Problemas maiores podem escalar pior
   - EficiÃªncia Ã© funÃ§Ã£o do tamanho do problema

### **ğŸ”® PrÃ³ximos Passos e OtimizaÃ§Ãµes**

#### **OtimizaÃ§Ãµes de Algoritmo**
- **Cache blocking**: Dividir computaÃ§Ã£o em blocos que cabem no cache
- **Loop tiling**: Melhorar localidade temporal dos dados
- **ParalelizaÃ§Ã£o temporal**: Explorar pipeline de iteraÃ§Ãµes temporais

#### **ExtensÃµes do Problema**
- **Grades 3D**: Expandir para simulaÃ§Ãµes tridimensionais
- **MÃºltiplas fases**: Adicionar pressÃ£o e termos convectivos
- **Adaptatividade**: Refino automÃ¡tico de grade em regiÃµes crÃ­ticas

#### **Tecnologias AvanÃ§adas**
- **GPU Computing**: CUDA ou OpenACC para aceleraÃ§Ã£o massiva
- **ComputaÃ§Ã£o distribuÃ­da**: MPI para simulaÃ§Ãµes muito grandes
- **Precision tuning**: AnÃ¡lise de precisÃ£o numÃ©rica vs performance

### **ğŸ› ï¸ Valor Educacional Demonstrado**

Este projeto exemplifica perfeitamente a integraÃ§Ã£o entre:
- **Teoria matemÃ¡tica** (EDPs, mÃ©todos numÃ©ricos)
- **ImplementaÃ§Ã£o prÃ¡tica** (C, OpenMP, algoritmos)
- **AnÃ¡lise quantitativa** (profiling, mÃ©tricas, visualizaÃ§Ã£o)
- **ValidaÃ§Ã£o fÃ­sica** (comportamento realista do fluido)

**Resultado**: Uma base sÃ³lida para compreender tanto os fundamentos teÃ³ricos quanto as consideraÃ§Ãµes prÃ¡ticas da computaÃ§Ã£o cientÃ­fica paralela.

## ğŸ› ï¸ **Troubleshooting e Comandos Ãšteis**

### **âŒ Problemas Comuns e SoluÃ§Ãµes**

#### **Erro de CompilaÃ§Ã£o: "undefined reference to pascal_start"**
```bash
# âŒ Problema: Biblioteca PaScal nÃ£o encontrada
# âœ… SoluÃ§Ã£o: Compilar com caminhos corretos
gcc -O2 -fopenmp -DUSE_PASCAL 
    -I$(pwd)/pascal-releases-master/include 
    -L$(pwd)/pascal-releases-master/lib 
    tarefa11_simples.c -lmpascalops -lm -o tarefa11_simples_pascal
```

#### **Erro: "Pascal not running" durante execuÃ§Ã£o**
```bash
# âŒ Problema: Normal quando executado diretamente (nÃ£o Ã© erro)
# âœ… SoluÃ§Ã£o: Para anÃ¡lise completa, usar pascalanalyzer:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal 
    --inst man --cors 1,2,4 --ipts "256 500" --outp resultado.json
```

#### **Erro: "Does not exist data of the sequential execution"**
```bash
# âŒ Problema: Pascal Viewer nÃ£o encontra dados seriais
# âœ… SoluÃ§Ã£o: Incluir 1 core na anÃ¡lise
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal 
    --inst man --cors 1,2,4,8 --ipts "256 500" --outp pascal_analysis.json
# Nota: O "1" em --cors Ã© crucial para o Pascal Viewer
```

#### **MemÃ³ria Insuficiente para Grades Grandes**
```bash
# âŒ Problema: Segmentation fault ou out of memory
# âœ… SoluÃ§Ãµes:
./tarefa11_simples 512 500     # Reduzir tamanho (em vez de 1024 2000)
ulimit -s unlimited           # Aumentar stack size (se necessÃ¡rio)
```

#### **Performance Inconsistente**
```bash
# âŒ Problema: Resultados variam muito entre execuÃ§Ãµes
# âœ… SoluÃ§Ãµes:
export OMP_PROC_BIND=close    # Fixar threads aos cores
export OMP_PLACES=cores       # Usar cores fÃ­sicos
./tarefa11_simples 256 500    # Executar mÃºltiplas vezes para mÃ©dia
```

### **âš™ï¸ Comandos AvanÃ§ados e ConfiguraÃ§Ãµes**

#### **AnÃ¡lise RÃ¡pida (Desenvolvimento)**
```bash
# Teste rÃ¡pido com poucos cores e iteraÃ§Ãµes
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal 
    --inst man 
    --cors 1,2,4 
    --ipts "128 100","256 200" 
    --rpts 1 
    --outp teste_rapido.json 
    --verb WARNING
```

#### **AnÃ¡lise Completa (ProduÃ§Ã£o)**
```bash
# AnÃ¡lise exaustiva para paper ou relatÃ³rio
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal 
    --inst man 
    --cors 1,2,4,8,16 
    --ipts "128 100","256 500","512 1000","1024 2000" 
    --rpts 5 
    --idtm 10 
    --outp analise_completa.json 
    --verb INFO
```

#### **AnÃ¡lise de Escalabilidade EspecÃ­fica**
```bash
# Focar apenas em uma configuraÃ§Ã£o de entrada
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/pascal-releases-master/lib
./pascal-releases-master/bin/pascalanalyzer ./tarefa11_simples_pascal 
    --inst man 
    --cors 1,2,4,6,8,12,16 
    --ipts "512 1000" 
    --rpts 3 
    --outp escalabilidade_512.json 
    --verb INFO
```

### **ğŸ”§ ConfiguraÃ§Ãµes de Ambiente**

#### **OtimizaÃ§Ã£o do Sistema para Performance**
```bash
# Desabilitar CPU frequency scaling
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Configurar OpenMP
export OMP_NUM_THREADS=8
export OMP_PROC_BIND=close
export OMP_PLACES=cores

# Configurar heap e stack
ulimit -s unlimited
export MALLOC_ARENA_MAX=1
```

#### **Monitoramento de Sistema Durante ExecuÃ§Ã£o**
```bash
# Em terminal separado, monitorar uso de CPU
watch -n 1 'top -p $(pgrep tarefa11_simples)'

# Ou usar htop para visualizaÃ§Ã£o mais rica
htop

# Para anÃ¡lise detalhada de cache misses (se disponÃ­vel)
perf stat -e cache-misses,cache-references ./tarefa11_simples 512 1000
```

### **ğŸ“Š Scripts Auxiliares**

#### **Script para MÃºltiplas ExecuÃ§Ãµes**
```bash
#!/bin/bash
# multiple_runs.sh - Executa programa mÃºltiplas vezes
for i in {1..5}; do
    echo "ExecuÃ§Ã£o $i:"
    ./tarefa11_simples 256 500 | grep "Tempo"
    echo "---"
done
```

#### **Script para AnÃ¡lise de Diferentes Tamanhos**
```bash
#!/bin/bash
# size_analysis.sh - Testa diferentes tamanhos de grade
sizes=(128 256 512 1024)
iters=(100 500 1000 2000)

for i in "${!sizes[@]}"; do
    size=${sizes[$i]}
    iter=${iters[$i]}
    echo "Testando ${size}x${size}, ${iter} iter:"
    ./tarefa11_simples $size $iter | tail -10
    echo "========================"
done
```

---

âœ¨ **Este README fornece uma documentaÃ§Ã£o completa e detalhada do projeto `tarefa11_simples.c`, cobrindo todos os aspectos solicitados: teoria, implementaÃ§Ã£o, resultados, comandos de compilaÃ§Ã£o e anÃ¡lise profunda do impacto das clÃ¡usulas schedule e collapse no desempenho paralelo.** âœ¨
