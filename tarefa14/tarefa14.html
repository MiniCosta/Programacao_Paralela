<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tarefa 14: Programação em Memória Distribuída</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            background-color: #fafafa;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            border-bottom: 1px solid #bdc3c7;
            padding-bottom: 5px;
            margin-top: 40px;
        }
        
        h3 {
            color: #2980b9;
        }
        
        h4 {
            color: #16a085;
        }
        
        code {
            background-color: #ecf0f1;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 0.9em;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
            color: inherit;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            background-color: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        th, td {
            border: 1px solid #bdc3c7;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        .highlight-box {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        
        .warning-box {
            background-color: #fef9e7;
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        
        .success-box {
            background-color: #eafaf1;
            border-left: 4px solid #27ae60;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        
        .results-section {
            background-color: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 30px 0;
        }
        
        .graph-container {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .graph-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        
        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .metric-card {
            background-color: white;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-left: 4px solid #e74c3c;
        }
        
        .regime-latencia {
            border-left-color: #e74c3c;
        }
        
        .regime-transicao {
            border-left-color: #f39c12;
        }
        
        .regime-banda {
            border-left-color: #27ae60;
        }
        
        .function-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .function-card {
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-left: 4px solid #9b59b6;
        }
        
        .toc {
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 20px 0;
        }
        
        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        
        .toc li {
            margin: 5px 0;
        }
        
        .toc a {
            color: #3498db;
            text-decoration: none;
        }
        
        .toc a:hover {
            text-decoration: underline;
        }
        
        .footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #bdc3c7;
            text-align: center;
            color: #7f8c8d;
        }
        
        ul li {
            margin: 8px 0;
        }
        
        ol li {
            margin: 8px 0;
        }
    </style>
</head>
<body>
    <h1>Tarefa 14: Programação em Memória Distribuída</h1>
    
    <div class="toc">
        <h3>Índice</h3>
        <ul>
            <li><a href="#descricao">Descrição</a></li>
            <li><a href="#sobre-mpi">Sobre MPI</a></li>
            <li><a href="#funcoes-mpi">Funções MPI Utilizadas</a></li>
            <li><a href="#padroes-comunicacao">Padrões de Comunicação</a></li>
            <li><a href="#tipos-send">Tipos de Send em MPI</a></li>
            <li><a href="#parametros">Detalhamento dos Parâmetros</a></li>
            <li><a href="#resultados">Resultados</a></li>
            <li><a href="#interpretacao">Interpretação dos Resultados</a></li>
            <li><a href="#conclusoes">Conclusões</a></li>
            <li><a href="#codigo">Código Fonte</a></li>
        </ul>
    </div>

    <h2 id="descricao">Descrição</h2>
    <div class="highlight-box">
        <p>Este programa implementa um teste de ping-pong MPI entre dois processos para medir latência e largura de banda em função do tamanho da mensagem.</p>
    </div>

    <h2 id="sobre-mpi">Sobre MPI (Message Passing Interface)</h2>

    <h3>O que é MPI?</h3>
    <p>MPI é um padrão de comunicação para programação paralela em sistemas de memória distribuída. Ele define uma API que permite que processos executando em diferentes nós de um cluster (ou mesmo na mesma máquina) se comuniquem através de troca de mensagens.</p>

    <h3>Características do MPI:</h3>
    <ul>
        <li><strong>Portabilidade:</strong> Funciona em diferentes arquiteturas e sistemas operacionais</li>
        <li><strong>Escalabilidade:</strong> Suporte desde poucos processos até milhares de processos</li>
        <li><strong>Flexibilidade:</strong> Diferentes padrões de comunicação (ponto-a-ponto, coletiva)</li>
        <li><strong>Performance:</strong> Otimizado para comunicação de alta performance</li>
    </ul>

    <h2 id="funcoes-mpi">Funções MPI Utilizadas</h2>

    <div class="function-grid">
        <div class="function-card">
            <h4>Inicialização e Finalização</h4>
            <p><strong><code>MPI_Init(&argc, &argv)</code></strong></p>
            <ul>
                <li>Deve ser a primeira chamada MPI</li>
                <li>Configura ambiente de comunicação</li>
            </ul>
            <p><strong><code>MPI_Finalize()</code></strong></p>
            <ul>
                <li>Deve ser a última chamada MPI</li>
                <li>Libera recursos e limpa estruturas</li>
            </ul>
        </div>

        <div class="function-card">
            <h4>Identificação</h4>
            <p><strong><code>MPI_Comm_rank(MPI_COMM_WORLD, &rank)</code></strong></p>
            <ul>
                <li>Obtém ID único do processo (0, 1, 2...)</li>
                <li>MPI_COMM_WORLD inclui todos os processos</li>
            </ul>
            <p><strong><code>MPI_Comm_size(MPI_COMM_WORLD, &size)</code></strong></p>
            <ul>
                <li>Obtém número total de processos</li>
            </ul>
        </div>

        <div class="function-card">
            <h4>Comunicação</h4>
            <p><strong><code>MPI_Ssend(buffer, count, datatype, dest, tag, comm)</code></strong></p>
            <ul>
                <li><strong>IMPORTANTE:</strong> Só completa quando MPI_Recv iniciou</li>
                <li>Garante sincronização real</li>
            </ul>
            <p><strong><code>MPI_Recv(buffer, count, datatype, source, tag, comm, status)</code></strong></p>
            <ul>
                <li>Sempre bloqueante</li>
                <li>Espera até mensagem chegar</li>
            </ul>
        </div>

        <div class="function-card">
            <h4>Temporização</h4>
            <p><strong><code>MPI_Wtime()</code></strong></p>
            <ul>
                <li>Tempo de alta precisão em segundos</li>
                <li>Resolução em microssegundos</li>
                <li>Monotônico - não afetado por mudanças no relógio</li>
                <li>Ideal para benchmarks</li>
            </ul>
        </div>
    </div>

    <h3>Padrão Ping-Pong</h3>
    <p>O teste ping-pong é um benchmark clássico para medir:</p>
    <ul>
        <li><strong>Latência:</strong> Tempo mínimo para enviar uma mensagem pequena</li>
        <li><strong>Largura de Banda:</strong> Taxa máxima de transferência para mensagens grandes</li>
        <li><strong>Overhead:</strong> Custo fixo da comunicação independente do tamanho da mensagem</li>
    </ul>

    <h2 id="padroes-comunicacao">Padrões de Comunicação: Bloqueante vs Não-Bloqueante</h2>

    <h3>Comunicação Bloqueante</h3>
    <div class="warning-box">
        <p>Uma operação <strong>bloqueante</strong> não retorna até que seja <strong>seguro</strong> reusar os recursos envolvidos (buffers, variáveis).</p>
    </div>

    <h4>Características:</h4>
    <ul>
        <li>✅ <strong>Segurança:</strong> Garante que a operação foi completada</li>
        <li>✅ <strong>Simplicidade:</strong> Programação mais intuitiva</li>
        <li>❌ <strong>Performance:</strong> Pode causar espera desnecessária</li>
        <li>❌ <strong>Deadlocks:</strong> Risco se não bem planejado</li>
    </ul>

    <h4>Exemplos de Funções Bloqueantes:</h4>
    <pre><code>// MPI_Send - pode ser bloqueante ou não dependendo da implementação
MPI_Send(buffer, count, MPI_INT, dest, tag, MPI_COMM_WORLD);

// MPI_Ssend - SEMPRE bloqueante (synchronous)
MPI_Ssend(buffer, count, MPI_INT, dest, tag, MPI_COMM_WORLD);

// MPI_Recv - SEMPRE bloqueante
MPI_Recv(buffer, count, MPI_INT, source, tag, MPI_COMM_WORLD, &status);</code></pre>

    <h4>Exemplo Prático - Ping-Pong Bloqueante:</h4>
    <pre><code>if (rank == 0) {
    MPI_Send(data, 100, MPI_INT, 1, 0, MPI_COMM_WORLD);     // Envia para processo 1
    MPI_Recv(data, 100, MPI_INT, 1, 0, MPI_COMM_WORLD, &status); // Recebe de volta
} else if (rank == 1) {
    MPI_Recv(data, 100, MPI_INT, 0, 0, MPI_COMM_WORLD, &status); // Recebe do processo 0
    MPI_Send(data, 100, MPI_INT, 0, 0, MPI_COMM_WORLD);     // Envia de volta
}</code></pre>

    <h3>Comunicação Não-Bloqueante</h3>
    <div class="success-box">
        <p>Uma operação <strong>não-bloqueante</strong> retorna <strong>imediatamente</strong>, iniciando a operação em background.</p>
    </div>

    <h4>Características:</h4>
    <ul>
        <li>✅ <strong>Performance:</strong> Permite sobreposição de computação e comunicação</li>
        <li>✅ <strong>Flexibilidade:</strong> Múltiplas operações simultâneas</li>
        <li>❌ <strong>Complexidade:</strong> Requer gerenciamento de requests</li>
        <li>❌ <strong>Sincronização:</strong> Necessita verificar completude com Wait/Test</li>
    </ul>

    <h4>Exemplos de Funções Não-Bloqueantes:</h4>
    <pre><code>MPI_Request request;
MPI_Status status;

// MPI_Isend - send não-bloqueante (immediate send)
MPI_Isend(buffer, count, MPI_INT, dest, tag, MPI_COMM_WORLD, &request);

// MPI_Irecv - recv não-bloqueante (immediate receive)
MPI_Irecv(buffer, count, MPI_INT, source, tag, MPI_COMM_WORLD, &request);

// Verificar se completou
MPI_Wait(&request, &status);  // Espera completar
// ou
int flag;
MPI_Test(&request, &flag, &status);  // Verifica sem esperar</code></pre>

    <h4>Exemplo Prático - Ping-Pong Não-Bloqueante:</h4>
    <pre><code>MPI_Request send_req, recv_req;
MPI_Status status;

if (rank == 0) {
    // Inicia send não-bloqueante
    MPI_Isend(data, 100, MPI_INT, 1, 0, MPI_COMM_WORLD, &send_req);
    // Inicia recv não-bloqueante
    MPI_Irecv(data, 100, MPI_INT, 1, 1, MPI_COMM_WORLD, &recv_req);
    
    // Pode fazer outras computações aqui...
    
    // Espera ambas as operações completarem
    MPI_Wait(&send_req, &status);
    MPI_Wait(&recv_req, &status);
} else if (rank == 1) {
    MPI_Irecv(data, 100, MPI_INT, 0, 0, MPI_COMM_WORLD, &recv_req);
    MPI_Isend(data, 100, MPI_INT, 0, 1, MPI_COMM_WORLD, &send_req);
    
    MPI_Wait(&recv_req, &status);
    MPI_Wait(&send_req, &status);
}</code></pre>

    <h2 id="tipos-send">Tipos de Send em MPI</h2>

    <div class="metrics">
        <div class="metric-card">
            <h4>1. MPI_Send (Standard Send)</h4>
            <pre><code>MPI_Send(buffer, count, datatype, dest, tag, comm);</code></pre>
            <ul>
                <li><strong>Semântica:</strong> Pode ser bloqueante ou não-bloqueante</li>
                <li><strong>Implementação:</strong> Depende do tamanho da mensagem</li>
                <li><strong>Uso:</strong> Geral, sem garantias específicas</li>
            </ul>
        </div>

        <div class="metric-card">
            <h4>2. MPI_Ssend (Synchronous Send)</h4>
            <pre><code>MPI_Ssend(buffer, count, datatype, dest, tag, comm);</code></pre>
            <ul>
                <li><strong>Semântica:</strong> SEMPRE bloqueante</li>
                <li><strong>Garantia:</strong> Só completa quando MPI_Recv iniciou</li>
                <li><strong>Uso:</strong> Sincronização garantida</li>
            </ul>
        </div>

        <div class="metric-card">
            <h4>3. MPI_Bsend (Buffered Send)</h4>
            <pre><code>MPI_Bsend(buffer, count, datatype, dest, tag, comm);</code></pre>
            <ul>
                <li><strong>Semântica:</strong> Não-bloqueante (se buffer disponível)</li>
                <li><strong>Requisito:</strong> Buffer alocado com MPI_Buffer_attach</li>
                <li><strong>Uso:</strong> Garantir não-bloqueio</li>
            </ul>
        </div>
    </div>

    <h3>Por que MPI_Ssend ao invés de MPI_Send?</h3>

    <h4>Diferenças entre os tipos de Send:</h4>
    <div class="highlight-box">
        <p><strong>1. MPI_Send (Standard Send):</strong></p>
        <ul>
            <li>Semântica <strong>não-determinística</strong></li>
            <li>Para mensagens pequenas: geralmente buffered (não-bloqueante)</li>
            <li>Para mensagens grandes: geralmente synchronous (bloqueante)</li>
            <li><strong>Problema:</strong> Comportamento inconsistente para diferentes tamanhos</li>
        </ul>
    </div>

    <div class="success-box">
        <p><strong>2. MPI_Ssend (Synchronous Send):</strong></p>
        <ul>
            <li><strong>SEMPRE bloqueante</strong> - só completa quando recv iniciou</li>
            <li>Comportamento <strong>determinístico</strong> independente do tamanho</li>
            <li><strong>Vantagem:</strong> Mede latência real de comunicação</li>
            <li><strong>Ideal para benchmarks</strong> de performance de rede</li>
        </ul>
    </div>

    <h4>Benefícios para este benchmark:</h4>
    <ul>
        <li>✅ <strong>Medição precisa:</strong> Elimina variações causadas por buffering interno</li>
        <li>✅ <strong>Comportamento consistente:</strong> Mesmo padrão para todos os tamanhos</li>
        <li>✅ <strong>Latência real:</strong> Mede tempo de comunicação efetiva entre processos</li>
        <li>✅ <strong>Resultados comparáveis:</strong> Entre diferentes implementações MPI</li>
    </ul>

    <h2 id="parametros">Detalhamento dos Parâmetros das Funções MPI</h2>

    <h3>Exemplo de chamada MPI_Ssend:</h3>
    <pre><code>MPI_Ssend(buffer, tam, MPI_CHAR, 1, 0, MPI_COMM_WORLD);</code></pre>
    <ul>
        <li><code>buffer</code>: Ponteiro para o array de dados a enviar</li>
        <li><code>tam</code>: Número de caracteres a enviar (tamanho da mensagem)</li>
        <li><code>MPI_CHAR</code>: Tipo de dados = char (1 byte por elemento)</li>
        <li><code>1</code>: Rank do processo destino (processo 1)</li>
        <li><code>0</code>: Tag da mensagem (identificador, pode ser qualquer inteiro)</li>
        <li><code>MPI_COMM_WORLD</code>: Comunicador global (todos os processos)</li>
    </ul>

    <h3>Exemplo de chamada MPI_Recv:</h3>
    <pre><code>MPI_Recv(buffer, tam, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);</code></pre>
    <ul>
        <li><code>buffer</code>: Ponteiro para o array que receberá os dados</li>
        <li><code>tam</code>: Número máximo de caracteres a receber</li>
        <li><code>MPI_CHAR</code>: Tipo de dados esperado = char</li>
        <li><code>0</code>: Rank do processo remetente (processo 0)</li>
        <li><code>0</code>: Tag da mensagem esperada (deve coincidir com o send)</li>
        <li><code>MPI_COMM_WORLD</code>: Comunicador</li>
        <li><code>MPI_STATUS_IGNORE</code>: Não precisamos das informações de status</li>
    </ul>

    <h3>Tags e Comunicadores:</h3>
    <ul>
        <li><strong>Tags:</strong> Permitem distinguir diferentes tipos de mensagens entre os mesmos processos</li>
        <li><strong>Comunicadores:</strong> Definem grupos de processos que podem se comunicar</li>
        <li><strong>MPI_COMM_WORLD:</strong> Comunicador padrão que inclui todos os processos do programa</li>
    </ul>

    <div class="results-section">
        <h2 id="resultados">Resultados</h2>
        
        <h3>Dados Coletados - Resultados com MPI_Ssend</h3>
        <p>O programa testou tamanhos de mensagem de 8 bytes até 1 MB (1.048.576 bytes), realizando 10.000 repetições para cada tamanho usando <strong>MPI_Ssend</strong> para garantir sincronização.</p>
        
        <table>
            <thead>
                <tr>
                    <th>Tamanho (bytes)</th>
                    <th>Tempo médio (μs)</th>
                    <th>Tempo total (s)</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>8</td><td>1.01</td><td>0.010058</td></tr>
                <tr><td>16</td><td>0.97</td><td>0.009663</td></tr>
                <tr><td>32</td><td>1.26</td><td>0.012645</td></tr>
                <tr><td>64</td><td>1.30</td><td>0.012959</td></tr>
                <tr><td>128</td><td>1.37</td><td>0.013721</td></tr>
                <tr><td>256</td><td>1.35</td><td>0.013512</td></tr>
                <tr><td>512</td><td>1.42</td><td>0.014150</td></tr>
                <tr><td>1024</td><td>1.41</td><td>0.014057</td></tr>
                <tr><td>2048</td><td>1.58</td><td>0.015825</td></tr>
                <tr><td>4096</td><td>4.30</td><td>0.042983</td></tr>
                <tr><td>8192</td><td>5.49</td><td>0.054875</td></tr>
                <tr><td>16384</td><td>7.44</td><td>0.074419</td></tr>
                <tr><td>32768</td><td>10.81</td><td>0.108077</td></tr>
                <tr><td>65536</td><td>17.39</td><td>0.173949</td></tr>
                <tr><td>131072</td><td>30.56</td><td>0.305616</td></tr>
                <tr><td>262144</td><td>51.87</td><td>0.518749</td></tr>
                <tr><td>524288</td><td>101.23</td><td>1.012320</td></tr>
                <tr><td>1048576</td><td>224.12</td><td>2.241173</td></tr>
            </tbody>
        </table>

        <div class="graph-container">
            <h3>Análise Gráfica: Latência vs Largura de Banda</h3>
            <img src="tarefa14_analise.png" alt="Gráfico de análise de latência vs largura de banda" />
            <p><em>Gráfico mostrando os regimes de latência e largura de banda em função do tamanho da mensagem</em></p>
        </div>

        <h3>Análise dos Regimes com MPI_Ssend</h3>
        
        <div class="metrics">
            <div class="metric-card regime-latencia">
                <h4>1. Regime de Latência (mensagens ≤ 2KB)</h4>
                <ul>
                    <li><strong>Características:</strong> Tempo dominado pela latência de comunicação sincronizada</li>
                    <li><strong>Observação:</strong> Tempo constante (~1.0-1.6 μs)</li>
                    <li><strong>Causa:</strong> Overhead do protocolo MPI_Ssend</li>
                    <li><strong>Diferença:</strong> Latência ~2.5x maior que MPI_Send</li>
                </ul>
            </div>
            
            <div class="metric-card regime-transicao">
                <h4>2. Regime de Transição (2KB - 4KB)</h4>
                <ul>
                    <li><strong>Características:</strong> Início da influência do tamanho</li>
                    <li><strong>Observação:</strong> Aumento no tempo (1.58 → 4.30 μs)</li>
                    <li><strong>Causa:</strong> Latência + tempo de transferência</li>
                </ul>
            </div>
            
            <div class="metric-card regime-banda">
                <h4>3. Regime de Largura de Banda (mensagens > 4KB)</h4>
                <ul>
                    <li><strong>Características:</strong> Tempo cresce linearmente</li>
                    <li><strong>Observação:</strong> Crescimento proporcional ao tamanho</li>
                    <li><strong>Causa:</strong> Largura de banda é o fator limitante</li>
                </ul>
            </div>
        </div>

        <h3>Explicação da Mudança de Dominância entre Regimes</h3>
        <div class="warning-box">
            <p>A mudança de dominância entre o regime de latência e o regime de largura de banda é um fenômeno fundamental em comunicação MPI que pode ser explicado pela natureza dos custos envolvidos na transmissão de dados:</p>
            
            <h4>1. Análise dos Custos de Comunicação</h4>
            <p>Toda comunicação MPI possui dois componentes principais de tempo:</p>
            <ul>
                <li><strong>Custo fixo (latência α):</strong> Tempo necessário para estabelecer a comunicação, processar headers, sincronizar processos e realizar handshakes entre remetente e destinatário. Este custo é independente do tamanho da mensagem.</li>
                <li><strong>Custo variável (largura de banda β×n):</strong> Tempo proporcional ao tamanho da mensagem, determinado pela velocidade de transferência efetiva dos dados através do meio de comunicação.</li>
            </ul>
            
            <h4>2. Comportamento nos Diferentes Regimes</h4>
            <p><strong>No regime de latência (mensagens pequenas ≤ 2KB):</strong></p>
            <ul>
                <li>O termo α (latência) domina a equação T(n) = α + β×n</li>
                <li>Como n é pequeno, β×n é desprezível comparado a α</li>
                <li>Resultado: tempo praticamente constante (~1.0-1.6 μs) independente do tamanho</li>
                <li>O overhead de estabelecimento da comunicação representa a maior parte do tempo total</li>
            </ul>
            
            <p><strong>No regime de largura de banda (mensagens grandes > 4KB):</strong></p>
            <ul>
                <li>O termo β×n passa a dominar a equação T(n) = α + β×n</li>
                <li>Como n é grande, β×n >> α</li>
                <li>Resultado: tempo cresce linearmente com o tamanho da mensagem</li>
                <li>A velocidade de transferência dos dados torna-se o fator limitante</li>
            </ul>
            
            <h4>3. O Ponto de Transição (2-4KB)</h4>
            <p>A transição ocorre quando α ≈ β×n, ou seja, quando os custos fixos e variáveis se tornam comparáveis. Neste experimento:</p>
            <ul>
                <li>Para 2KB: tempo = 1.58 μs (ainda dominado pela latência)</li>
                <li>Para 4KB: tempo = 4.30 μs (início da dominância da largura de banda)</li>
                <li>O salto de 2.7x no tempo indica que o sistema começou a ser limitado pela transferência de dados</li>
            </ul>
            
            <h4>4. Fatores Técnicos que Influenciam a Transição</h4>
            <ul>
                <li><strong>Buffers internos:</strong> Mensagens pequenas podem ser copiadas para buffers internos do MPI, enquanto mensagens grandes requerem transferência direta</li>
                <li><strong>Protocolos de transferência:</strong> MPI pode usar diferentes estratégias (eager vs rendezvous protocol) baseadas no tamanho da mensagem</li>
                <li><strong>Sincronização MPI_Ssend:</strong> A garantia de sincronização adiciona overhead constante, mas torna a medição mais precisa</li>
                <li><strong>Características do hardware:</strong> A largura de banda efetiva da memória e do sistema de interconexão determina o coeficiente β</li>
            </ul>
            
            <h4>5. Implicações Práticas</h4>
            <p>Compreender esta transição é crucial para:</p>
            <ul>
                <li><strong>Otimização de aplicações:</strong> Mensagens pequenas devem ser agrupadas para amortizar a latência</li>
                <li><strong>Escolha de algoritmos:</strong> Diferentes estratégias são eficientes para diferentes tamanhos de dados</li>
                <li><strong>Predição de performance:</strong> O modelo α-β permite estimar tempos de comunicação</li>
                <li><strong>Tuning de sistemas:</strong> Identificar gargalos de latência vs largura de banda</li>
            </ul>
        </div>

        <h3>Métricas Importantes - Versão MPI_Ssend</h3>
        <div class="highlight-box">
            <ul>
                <li><strong>Latência base:</strong> ~1.0 μs (para mensagens pequenas)</li>
                <li><strong>Largura de banda efetiva:</strong> ~4.7 GB/s (calculada para mensagem de 1MB)</li>
                <li><strong>Ponto de transição:</strong> ~2-4KB (onde a largura de banda começa a dominar)</li>
                <li><strong>Overhead de sincronização:</strong> ~0.6 μs comparado ao MPI_Send padrão</li>
            </ul>
        </div>

        <h3>Observações Importantes:</h3>
        <ol>
            <li><strong>MPI_Ssend é mais lento:</strong> Latência base ~2.5x maior que MPI_Send</li>
            <li><strong>Sincronização garantida:</strong> Elimina variações por buffering</li>
            <li><strong>Comportamento consistente:</strong> Mesmo padrão para todos os tamanhos</li>
            <li><strong>Ideal para benchmarks:</strong> Medição precisa da latência real de comunicação</li>
        </ol>
    </div>

    <h2 id="conclusoes">Conclusões</h2>
    <ol>
        <li>Para mensagens pequenas (≤1KB), a latência de comunicação é o fator dominante</li>
        <li>Para mensagens grandes (>1KB), a largura de banda se torna limitante</li>
        <li>O sistema MPI local apresenta excelente performance com latência muito baixa</li>
        <li>A transição entre regimes ocorre em torno de 1-4KB, típico para sistemas de comunicação local</li>
        <li>O modelo α-β ajuda a prever performance para diferentes tamanhos de mensagem</li>
        <li><strong>MPI_Ssend fornece medições mais precisas</strong> para benchmarks de comunicação</li>
        <li>A sincronização garantida elimina incertezas sobre políticas de buffering</li>
    </ol>

    <h2 id="codigo">Código Fonte</h2>
    <div class="graph-container">
        <h3>Implementação do Programa MPI Ping-Pong</h3>
        <img src="tarefa14.png" alt="Código fonte do programa tarefa14.c" />
        <p><em>Código fonte completo do programa implementado em C com MPI_Ssend</em></p>
    </div>

</body>
</html>
